{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"top\"></a>\n",
    "# Table of Contents\n",
    "- [3. Unsupervised Learning](#3)\n",
    "    - [3.1 Definitions and Terminology](#3_1)\n",
    "    - [3.2 Preparing Data for Unsupervised Learning](#3_2)\n",
    "    - [3.3 Types of Unsupervised Learning](#3_3)\n",
    "        - [3.3.1 Clustering](#3_3_1)\n",
    "            - [K-Means Clustering](#3_3_1_1)\n",
    "            - [Hierarchical Clustering](#3_3_1_2)\n",
    "            - [DBSCAN](#3_3_1_3)\n",
    "        - [3.3.2 Dimensionality Reduction](#3_3_2)\n",
    "            - [PCA](#3_3_2_1)\n",
    "            - [t-SNE](#3_3_2_2)\n",
    "    - [3.4 Advantages and Challenges](#3_4)\n",
    "    - [3.5 Exercises](#3_5)\n",
    "    - [3.6 Learning Material](#3_6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Unsupervised Learning<a name=\"3\"></a>\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img alt=\"AI\" src=\"../img/3/images.jpg\" width=200>\n",
    "            <A src=\"https://www.cartoonstock.com/\">cartoonstock</A>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img alt=\"AI\" src=\"../img/3/main.png\" width=200>\n",
    "            <A src=\"https://prateekvjoshi.com/tag/pattern-recognition/\">prateekvjoshi.com</A>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Definitions and Terminology<a name=\"3_1\"></a>\n",
    "\n",
    "### What is Unsupervised Learning?\n",
    "\n",
    "A class of machine learning (ML) used to **detect patterns** in data **without the use of any labels**.\n",
    "Only input variables (X) available, **no output variable (Y)**.\n",
    "\n",
    "**Unsupervised learning** algorithms discover *underlying hidden structures* in data. The learning process is independent. \n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Unsupervised_learning)\n",
    "\n",
    "<img alt=\"Algorithms\" src=\"../img/3/algos.png\" width=300>\n",
    "\n",
    "Source: [twitter](https://twitter.com/fuzzysynapse/status/930716639041146880)\n",
    "\n",
    "> Unsupervised learning — teaching machines to learn for themselves without having to be explicitly told if everything \n",
    "> they do is right or wrong — is the key to “true” AI.\" <cite> Yann Lecun </cite>\n",
    "\n",
    "![UL](../img/3/UL.jpg)\n",
    "<A src=\"https://www.sem-deutschland.de/inbound-marketing-agentur/online-marketing-glossar/was-ist-maschinen-learning-definition-funktionsweise-bedeutung/\"> www.sem-deutschland.de</A>\n",
    "\n",
    "#### An example: \n",
    "If you take a test, there are questions and answers. Your grade is determined by how closely your answers match the answer key. But what if there was no answer key? How would you grade yourself?\n",
    "\n",
    "In ML, datasets typically follow logic X -> Y. We might want to find if increase in ice-cream sales (X) cause a decrease in crime rates (Y), for instance. In unsupervised learning, there are no crime rates available! We just have the input data X.\n",
    "\n",
    "Important Terminology:\n",
    "\n",
    "* Feature: An *input variable* ($X$) used in making predictions.\n",
    "* Predictions: A model's *output* ($\\hat{y}$), when provided with an input example.\n",
    "* Observation / example: One *row* of a data set. It has one or more features and possibly a label.\n",
    "* Label: Result of the feature  ($y$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Supervised vs Unsupervised Learning\n",
    "\n",
    "\n",
    "<img alt=\"machine_learning_class\" src=\"../img/3/ML.png\" width=350>\n",
    "\n",
    "Source: [blog.algorithmia.com](https://blog.algorithmia.com/introduction-to-unsupervised-learning/)\n",
    "\n",
    "- In supervised learning, previous examples of labelled datasets are used as the basis to learn from. \n",
    "\n",
    "- In unsupervised learning, attempts are made to **detect patterns** directly from the available example as no labelled datasets are provided. \n",
    "\n",
    "<img src=\"../img/3/supvsunsup.png\" alt=\"Supervised_vs_Unsupervised\" width=450>\n",
    "\n",
    "Source: [beta.cambridgespark.com](http://beta.cambridgespark.com/courses/jpm/01-module.html)\n",
    "\n",
    "The left image is an example of supervised learning; regression techniques are used to find the line of best fit between the features. In unsupervised learning, the inputs are segregated based on features and the prediction is based on which cluster it belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Why use Unsupervised Learning?\n",
    "\n",
    "<img alt=\"Customer Segmentation\" src=\"../img/3/customer-segmentation.jpg\" width=400>\n",
    "\n",
    "Source: [www.analyticsvidhya.com](https://www.analyticsvidhya.com/blog/2018/05/essentials-of-deep-learning-trudging-into-unsupervised-deep-learning/)\n",
    "\n",
    "Examples of where unsupervised learning is useful:\n",
    "- An advertising platform **segments the population into smaller groups** with similar demographics, lifestyles, and purchasing habits so that advertisers can reach their target market with relevant ads. This helps advertisers to understand their existing customer base and use their ad spend effectively by targeting potential new customers.\n",
    "- Companies like Airbnb **group housing listings** into neighborhoods so that users can navigate listings better. \n",
    "- A data science team **reduces the number of dimensions** in a large data set to simplify modelling and reduce file size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to top](#top)\n",
    "## 3.3 Types of Unsupervised Learning<a name=\"3_3\"></a>\n",
    "\n",
    "* Clustering\n",
    "* Dimensionality Reduction\n",
    "* Anomaly Detection\n",
    "* Autoencoders\n",
    "* Deep Belief Nets\n",
    "* Generative Adversarial Networks (GANs)\n",
    "- Self-Organizing maps\n",
    "\n",
    "Here we will focus on only two: clustering and dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.3.1 Clustering<a name=\"3_3_1\"></a>\n",
    "\n",
    "In clustering, the data gets divided into several groups or clusters according to similar features. Items in a cluster would be more similar to each other than items in other clusters. It's up to us to interpret the results: value is created when humans create meaning using the findings.\n",
    "\n",
    "Companies like ours need to understand their customers. Customers could be divided into groups according to some set criteria (age, gender, user history) using unsupervised learning. Clustering algorithms find natural groupings in data if they exist. For us, this might mean one cluster of 30-something traders and another of millennials who own hedge funds. We can specify how many clusters we want to find, hence modifying the granularity of these groups. \n",
    "\n",
    "![Clusters](../img/3/clusters.jpeg)\n",
    "\n",
    "Source: https://medium.com/the-21st-century/machine-learning-a-strategy-to-learn-and-understand-chapter-3-9daaad4afc55\n",
    "\n",
    "The left image shows raw data where the classification isn't done; the right image shows clustered data. A new input is classified into one of these clusters based on features and a prediction is made.\n",
    "\n",
    "There are several types of clustering. We'll explore two of the most common forms of clustering: k-means and hierarchical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate some data:  Make your own random clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# create blobs\n",
    "true_clusters = 4\n",
    "data = make_blobs(n_samples=200, n_features=2, centers=true_clusters, cluster_std=1.1)\n",
    "\n",
    "X, y = data[0],  data[1]\n",
    "\n",
    "# create scatter plot\n",
    "plt.scatter(X[:,0], X[:,1], c=y, edgecolor=\"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to top](#top)\n",
    "\n",
    "#### 3.3.1.1 K-Means Clustering <a name=\"3_3_1_1\"></a>\n",
    "\n",
    "Involves clustering data points into **K mutually exclusive clusters**. Choosing the desired number for K can be difficult. A larger K creates smaller groups with more granularity, a lower K means larger groups and less granularity.\n",
    "\n",
    "\n",
    "If we know that there are 3 classes involved, we set the algorithm to group the data into 3 classes. Randomly, 3 points are assigned to 3 clusters. Based on the centroid distance between points, the next inputs are segregated into the clusters. \n",
    "\n",
    "Each centroid of a cluster is a collection of feature values which define the resulting groups. The centroids are like the heart of the cluster, they “capture” the points closest to them and add them to the cluster. The centroids are recalculated for all the clusters. Think of these as the people who show up at a party and soon become the centers of attention because they’re so magnetic. If there’s just one of them, everyone will gather around; if there are lots, many smaller centers of activity will form. The output of the clustering algorithm would be a set of labels assigning each data point to one of the k groups. \n",
    "<table>\n",
    "    <tr>\n",
    "        <td width=30% style=\"text-align:left; LINE-HEIGHT:200%\">\n",
    "            Steps in k-means clustering:\n",
    "            <ol>\n",
    "              <li>Initialize k centroids at random.</li>\n",
    "              <li>Assign each data point to one of the k clusters. Measure of \"nearness\" is a hyperparameter — often Euclidean distance.</li>\n",
    "              <li>Move the centroids to the center of their respective clusters. The new position of each centroid is calculated as the average position of all the points in its cluster.</li>\n",
    "                <li>Keep repeating steps 2 and 3 until the centroids stop moving a lot at each iteration, until the algorithm converges.</li>\n",
    "            </ol>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../img/3/kmeans_algo.png\">\n",
    "\n",
    "            modified after this <A href=\"http://www.youtube.com/watch?v=_aWzGGNrcic\">youtube video</A>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Another application of k-means clustering is classifying handwritten digits. Suppose we have digits' images as vectors of pixel brightnesses. Let's say the images are black and white with 64x64 pixels. Each pixel represents a dimension. k-means clustering allows us to group the images that are close together, achieving good results for digit recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Modules\n",
    "from sklearn.cluster import KMeans\n",
    "import random\n",
    "\n",
    "\n",
    "K=8\n",
    "# Declaring Model\n",
    "model = KMeans(n_clusters=K)\n",
    "\n",
    "# Fitting Model\n",
    "model.fit(X)\n",
    "\n",
    "# Prediction on the entire data\n",
    "y_predict = model.predict(X)\n",
    "\n",
    "\n",
    "# make lists of colors for plotting\n",
    "random.seed(43)\n",
    "def random_color_list(length=10):\n",
    "    return [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in range(length)]\n",
    "\n",
    "random_colors_y = random_color_list(len(set(y)))\n",
    "random_colors_p = random_color_list(len(set(y_predict)))\n",
    "\n",
    "plt.title(\"K-Means clustering K=%d, true clusters=%d\" % (K, true_clusters))\n",
    "for xi, yi, cy, cp in zip(X[:,0], X[:,1], y, y_predict):\n",
    "    plt.scatter(xi, yi, c=random_colors_p[cp], edgecolors=random_colors_y[cy] , linewidths=2, alpha=0.8, s=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to top](#top)\n",
    "\n",
    "#### 3.3.1.2 Hierarchical Clustering<a name=\"3_3_1_2\"></a>\n",
    "\n",
    "1. **Bottom-up** or agglomerative clustering. \n",
    "    - This is the most common type of hierarchical clustering, \n",
    "    - It is agglomerative (a dendrogram is built starting from the leaves)\n",
    "2. **Top-down** or divisive clustering. \n",
    "    - It is divisive (a dendrogram is built starting from the trunk)\n",
    "\n",
    "<img src=\"../img/3/hclust.jpg\" width=600>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top Down\n",
    "All the data points are assigned to individual clusters. Then the 2 closest clusters are joined into one. The algorithm ends when there is only one single cluster left. The completion of hierarchical clustering is shown using **dendrograms**. \n",
    "\n",
    "It clusters data points into **parent** and **child clusters**. We can divide our customers into younger and older ages, and then split each of those groups into further individual clusters as well. In addition to cluster assignments, we also build a nice tree that tells us about the hierarchies between the clusters. We can then pick the number of clusters we want from this tree.\n",
    "\n",
    "Hierarchical clustering is similar to regular clustering, except that you build a hierarchy of clusters. This is useful if you want flexibility in how many clusters you ultimately want. For example, imagine grouping items on an online marketplace like Etsy or Amazon. On the homepage you'd want a few broad categories of items for simple navigation, but as you go into more specific categories, you'd want **increasing levels of granularity** or more distinct clusters of items.\n",
    "\n",
    "Here are the steps for hierarchical clustering:\n",
    "1. Start with N clusters, one for each data point.\n",
    "2. Merge the two clusters that are closest to each other. Now you have N-1 clusters.\n",
    "3. Recompute the distances between the clusters. There are several ways to do this. One of them (called average-linkage clustering) is to consider the distance between two clusters to be the average distance between all their respective members.\n",
    "4. Repeat steps 2 and 3 until you get one cluster of N data points. You get a tree (also known as a dendrogram) like the one below.\n",
    "5. Pick a number of clusters and draw a horizontal line in the dendrogram. For example, if you want k=2 clusters, you should draw a horizontal line around “distance=20000.” You’ll get one cluster with data points 8, 9, 11, 16 and one cluster with the rest of the data points. In general, the number of clusters you get is the number of intersection points of your horizontal line with the vertical lines in the dendrogram.\n",
    "\n",
    "Now we see an "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example of hierarchical clustering of grain data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Python Modules\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "# Reading the DataFrame\n",
    "seeds_df = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/vihar/unsupervised-learning-with-python/master/seeds-less-rows.csv\")\n",
    "\n",
    "# Remove the grain species from the DataFrame, save for later\n",
    "varieties = list(seeds_df.pop('grain_variety'))\n",
    "\n",
    "# Extract the measurements as a NumPy array\n",
    "samples = seeds_df.values\n",
    "\n",
    "\"\"\"\n",
    "Perform hierarchical clustering on samples using the\n",
    "linkage() function with the method='complete' keyword argument.\n",
    "Assign the result to mergings.\n",
    "\"\"\"\n",
    "mergings = linkage(samples, method='complete')\n",
    "\n",
    "\"\"\"\n",
    "Plot a dendrogram using the dendrogram() function on mergings,\n",
    "specifying the keyword arguments labels=varieties, leaf_rotation=90,\n",
    "and leaf_font_size=6.\n",
    "\"\"\"\n",
    "dendrogram(mergings,\n",
    "           labels=varieties,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=8,\n",
    "           )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Difference between K Means and Hierarchical clustering\n",
    "\n",
    "* Hierarchical clustering can't handle **big data** well but K means clustering can. This is because the time complexity of K means is linear i.e. $O(n)$ while that of hierarchical clustering is quadratic i.e. $O(n^2)$.\n",
    "\n",
    "* Results are **reproducible** in hierarchical clustering. In K means clustering, as we start with an arbitrary choice of clusters, the results generated by running the algorithm multiple times might differ. \n",
    "\n",
    "* K means is found to work well when the **shape of the clusters** is hyper spherical (like circle in 2D, sphere in 3D).\n",
    "\n",
    "* K-Means doesn't allow **noisy data**, while in hierarchical we can directly use noisy dataset for clustering.\n",
    "\n",
    "* Hierarchical clustering tends to produce **more accurate** results compared to k-means clustering. The downside is that hierarchical clustering is more difficult to implement and more time/resource consuming than k-means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to top](#top)\n",
    "\n",
    "#### 3.3.1.3 DBSCAN Clustering<a name=\"3_3_1_3\"></a>\n",
    "\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a popular clustering algorithm used as an replacement to K-means in predictive analytics. It doesn't require the number of clusters, but we need to provide **two other parameters**.\n",
    "\n",
    "The scikit-learn implementation provides a default for the **eps** and **min_samples parameters**, but we need to tune those. \n",
    "- The **eps parameter** is the maximum distance between two data points to be considered in the same neighborhood. \n",
    "- The **min_samples parameter** is the minimum amount of data points in a neighborhood to be considered a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interesting\n",
    "![supervised vs unsupervised](../img/3/sports.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Python Modules\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load Dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Declaring Model\n",
    "dbscan = DBSCAN()\n",
    "\n",
    "# Fitting\n",
    "dbscan.fit(iris.data)\n",
    "\n",
    "# Reducing Dimensions from 4 to 2 using PCA\n",
    "pca = PCA(n_components=2).fit(iris.data)\n",
    "pca_2d = pca.transform(iris.data)\n",
    "\n",
    "# Plot based on Class\n",
    "for i in range(0, pca_2d.shape[0]):\n",
    "    if dbscan.labels_[i] == 0:\n",
    "        c1 = plt.scatter(pca_2d[i, 0], pca_2d[i, 1], c='r', marker='+')\n",
    "    elif dbscan.labels_[i] == 1:\n",
    "        c2 = plt.scatter(pca_2d[i, 0], pca_2d[i, 1], c='g', marker='o')\n",
    "    elif dbscan.labels_[i] == -1:\n",
    "        c3 = plt.scatter(pca_2d[i, 0], pca_2d[i, 1], c='b', marker='*')\n",
    "\n",
    "plt.legend([c1, c2, c3], ['Cluster 0', 'Cluster 1', 'Noise (-1)'])\n",
    "plt.title('DBSCAN finds 2 clusters and Noise')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dbscan.labels_)\n",
    "print(iris.target)\n",
    "print(iris.target_names)\n",
    "\n",
    "print(len(dbscan.labels_))\n",
    "print(len(iris.target))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to top](#top)\n",
    "\n",
    "### 3.3.2 Dimensionality Reduction<a name=\"3_3_2\"></a>\n",
    "\n",
    "Running algorithms on only data that's necessary is sensible. Unsupervised learning can help with dimensionality reduction and reduce data complexity while maintaining data structure and usefulness. Reducing dimensionality of data is an important part of a good machine learning pipeline and resembles compression. In computer vision, reducing the size of training set by an order of magnitude by running algorithms on images would significantly lower compute and storage costs while making models run faster. \n",
    "\n",
    "Dimensionality reduction (dimensions or how many columns are in your dataset) assumes that a lot of data is redundant, and that most of the information in a dataset can be represented by only a fraction of it. This means combining parts of the data to convey maximum information. \n",
    "\n",
    "#### Algorithms to reduce dimensionality:\n",
    "\n",
    "- Principal Component Analysis (PCA): finds the linear combinations that conveys most of the variance in your data.\n",
    "- Singular-Value Decomposition (SVD): decomposes data into product of smaller matrices.\n",
    "\n",
    "These methods use linear algebra to break down a matrix into more informatory pieces.\n",
    "\n",
    "We'll take a look at one common technique in practice: principal component analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 3.3.2.1 Principal Components Analysis<a name=\"3_3_2_1\"></a>\n",
    "\n",
    "\n",
    "PCA is a statistical procedure that uses a transformation to convert a set of observations of possibly correlated variables  into a set of values of **linearly uncorrelated variables** called **principal components**.\n",
    "\n",
    "This transformation is defined in such a way that the **first principal component** has the *largest possible variance* , and **each succeeding component** in turn has the *highest variance possible* under the constraint that it is orthogonal to the preceding components. \n",
    "<A href=\"https://en.wikipedia.org/wiki/Principal_component_analysis\">wikipedia PCA</A>\n",
    "\n",
    "The subset we select will constitute a new space that is smaller in dimensionality than the original space but maintains as much of the complexity of the data as possible. PCA remaps the space in which our data exists.\n",
    "\n",
    "<img src=\"../img/3/pca_2comp.png\" width=500 alt=pca_2comp>\n",
    "\n",
    "The original Iris dataset has 4 columns: sepal length, sepal width, petal length, and petal width. The following code projects the original data which is 4 dimensional (${x_1,x_2,x_3,x_4}$) into 2 dimensions (${z_1,z_2}$). After dimensionality reduction, there isn’t a particular meaning assigned to each principal component typically. The new components are just the 2 main dimensions of variation. \n",
    "\n",
    "After plotting the data in the new dimensions(${z_1,z_2}$), the different classes seem well separated from each other.\n",
    "\n",
    "The **explained variance**  tells us how much information (variance) can be attributed to each of the principal components. This is important as converting 4 dimensional space to 2 dimensional space can lose some of the variance (information). Here, the first principal component contains 92.46% of the variance and the second principal component contains 5.30% of the variance. Together, the two components contain 97.76% of the total information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "# Load the iris data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Fit a PCA\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "pca.fit(X)\n",
    "\n",
    "# Percentage of variance explained for each components\n",
    "print('explained variance ratio (first two components): %s'\n",
    "      % str(pca.explained_variance_ratio_))\n",
    "\n",
    "# Project the data in 2D\n",
    "X_pca = pca.transform(X)\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure()\n",
    "lw = 2 \n",
    "target_ids = range(len(iris.target_names))\n",
    "plt.figure(figsize=(6, 5))\n",
    "for i, c, label in zip(target_ids, 'rgbcmykw', iris.target_names):\n",
    "    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1],\n",
    "               c=c, label=label)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('PCA of IRIS dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to top](#top)\n",
    "\n",
    "#### 3.3.2.2  t-SNE<a name=\"3_3_2_2\"></a>\n",
    "\n",
    "t-SNE (t-distributed stochastic neighbor embedding) is one of the unsupervised learning methods for visualisation. It maps high dimensional space into a 2 or 3 dimensional space which can be visualised. Specifically, it models each high-dimensional object by a 2 or 3-dimensional point in such a way that similar objects are modelled by nearby points and dissimilar objects are modelled by distant points with high prob 2-dimensional figure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Python Modules\n",
    "from sklearn import datasets\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Loading dataset\n",
    "iris_df = datasets.load_iris()\n",
    "\n",
    "# Defining Model\n",
    "model = TSNE(learning_rate=100, perplexity=5, n_components=2)\n",
    "\n",
    "# Fitting Model and transforming the data\n",
    "transformed = model.fit_transform(iris_df.data)\n",
    "\n",
    "# Plotting 2d t-Sne\n",
    "x_axis = transformed[:, 0]\n",
    "y_axis = transformed[:, 1]\n",
    "\n",
    "plt.scatter(x_axis, y_axis, c=iris_df.target)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to top](#top)\n",
    "\n",
    "## 3.4 Advantages and Challenges<a name=\"3_4\"></a>\n",
    "\n",
    "###  Advantages\n",
    "    \n",
    "* Supervised learning is **constrained by the biases** in which it is being supervised in; it cannot think of other corner cases that could occur when solving the problem.\n",
    "\n",
    "* Also, huge manual **effort is required to create labels** in supervised learning. The less the number of labels created, less is the training that can be perform for your algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Challenges \n",
    "\n",
    "Unsupervised learning presents a unique challenge: it's difficult to know if we're getting the right answers without an answer key. In supervised learning, metrics like precision and recall reveal how accurate our model is, and model parameters are tuned to improve accuracy scores. **With absence of labels in unsupervised learning, we can't objectively measure how accurate our algorithm is.** How do we know if K-Means found the right clusters or even the right number of clusters? \n",
    "\n",
    "If unsupervised learning is right for the job, depends on your business context. \"Performance\" is often subjective and domain-specific. In customer segmentation, clustering will only work well, if your customers actually do split into natural groups. One of the best but risky ways to test an unsupervised learning model is by implementing it in the real world and seeing what happens. Designing an A/B test, with and without the clusters our algorithm outputted, can be an effective way to see if it's useful information or totally incorrect.\n",
    "\n",
    "<img src=\"../img/3/robots.jpg\" alt=\"Robots\" width=300>\n",
    "\n",
    "Source: www.cartoonstock.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Any Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/3/AI.jpg\" width=300 alt=\"AI\">\n",
    "\n",
    "Source: [pinterest](https://www.pinterest.com/TechSeeAugmentedVision/artificial-intelligence/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to top](#top)\n",
    "\n",
    "## 3.5 Exercises<a name=\"3_5\"></a>\n",
    "1. K Means Clustering <A href=\"#3_3_1\">(go up)</A>  documentation on K-Means <A href=\"https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\">K-Means doc</A>: \n",
    "    - Play around with these parameters: **true_clusters** and **K**.  <br>\n",
    "    What happens if you guess the right number of clusters with **K=true_clusters** and **otherwise**?\n",
    "    - Change **cluster_std** when generating data. <br>This will make the clusters more dispersed and it will be more difficult for the model to separate them.\n",
    "    - K Means might give you **different results** each time you run it. <br>Try to observe this behaviour. How can you avoid this in your code? <br><br>\n",
    "2. t-SNE <a href=\"#3_3_2_2\">(go up)</a> documenation on t-SNE: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\"> sklearn doc</a>\n",
    "    - Play around with these parameters: **perplexity** and **learning rate**.  <br>\n",
    "    Find a combination of both that gives you the **best separation** of the iris species."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Learning Material<a name=\"3_6\"></a>\n",
    "- https://towardsdatascience.com/unsupervised-learning-with-python-173c51dc7f03\n",
    "- https://scikit-learn.org/stable/tutorial/statistical_inference/unsupervised_learning.html\n",
    "- https://www.analyticsvidhya.com/blog/2018/05/essentials-of-deep-learning-trudging-into-unsupervised-deep-learning/\n",
    "- https://www.coursera.org/lecture/machine-learning/unsupervised-learning-olRZo\n",
    "- https://www.datacamp.com/courses/unsupervised-learning-in-python\n",
    "- https://www.kaggle.com/sashr07/unsupervised-learning-tutorial\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
