{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Supervised Learning\n",
    "#### Regression vs Classification\n",
    "Repetition:\n",
    "Supervised learning problems can be sub-divided into regression and classification problems.\n",
    "\n",
    "**Regression** covers situations where **y is continuous/numerical**. \n",
    "- Predicting the value of the Dow in 6 months.\n",
    "- Predicting the price of a given house based on various inputs.\n",
    "\n",
    "**Classification** covers situations where **y is categorical**\n",
    "- Will the Dow be up (U) or down (D) in 6 months?\n",
    "- Is this email a SPAM or not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Regression\n",
    "- Linear regression is a simple approach to supervised learning\n",
    "- It assumes a linear dependence of Y on $X_1, X_2, ... X_n$\n",
    "- Linear Regression is very useful (conceptually and practically)\n",
    "\n",
    "![adv](../img/2/advertise.png)\n",
    "source: [ISLR](http://www-bcf.usc.edu/~gareth/ISL/)\n",
    "\n",
    "Questions linear regression can help us to answer:\n",
    "1. Is there a relationship between advertising budget and sales? \n",
    "2. How strong is the relationship between advertising budget and sales? \n",
    "3. Which factors (media) contribute to sales?\n",
    "4. How accurately can we estimate the effect of each medium on sales? \n",
    "5. How accurately can we predict future sales?\n",
    "6. Is the relationship linear?\n",
    "7. Is there synergy among the advertising media?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Simple Linear Regression\n",
    "Simple linear regression assumes there is a linear relationship between x and y.\n",
    "$$ y = \\beta_{0}  + \\beta_{1} x $$\n",
    "\n",
    "<img src=\"../img/2/simple_linear_slope.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's gererate some random data for regression analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# generate random data from a line adding some noise\n",
    "def generate_regr(n_samples, x_mean, x_std, m, b, y_noise):\n",
    "    x, y = list(), list()\n",
    "    for i in range(n_samples):\n",
    "        x0 = random.gauss(x_mean, x_std)\n",
    "        y0 = m * x0 + b + (random.gauss(0, y_noise))\n",
    "        x.append(x0)\n",
    "        y.append(y0)\n",
    "    return np.array([x]).transpose(), y\n",
    "\n",
    "m, b, noise = 1, 0, 1\n",
    "X, y = generate_regr(50, 0, 10, m, b, noise)\n",
    "\n",
    "# plot dataset\n",
    "pyplot.close()\n",
    "pyplot.scatter(X,y)\n",
    "\n",
    "# ----------- create a model and fit it to the data ----------\n",
    "regr_model = LinearRegression()\n",
    "regr_model.fit(X, y)\n",
    "\n",
    "# regression line\n",
    "xl = np.linspace(min(X), max(X), 100)\n",
    "yl = regr_model.coef_[0] * xl + regr_model.intercept_\n",
    "yt= m * xl + b\n",
    "\n",
    "pyplot.plot(xl, yl, '-r', label='linear regression')\n",
    "pyplot.plot(xl, yt, '-b', label='truth ')\n",
    "pyplot.legend(loc='upper left')\n",
    "pyplot.show()\n",
    "\n",
    "print(\"slope:\")\n",
    "print(\"  true:\", m, \" model: \", regr_model.coef_[0])\n",
    "print(\"intercept:\")\n",
    "print(\"  true:\", b, \" model: \", regr_model.intercept_)\n",
    "\n",
    "print(\"R-squared: \", regr_model.score(X, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How does this work?\n",
    "\n",
    "The optimization algorithm tries to find a line for which the **distance of the data points (residuals)** to this line is minimized. The way to *'punish'* this distance is called a **cost function**.\n",
    "\n",
    "![residuals](../img/2/residuals.png)\n",
    "\n",
    "source: [ISLR](http://www-bcf.usc.edu/~gareth/ISL/)\n",
    "\n",
    "\n",
    "There are different **cost funtions** to pick from \n",
    "- RSS: Residual Sum of Squares\n",
    "$$  \\sum_i (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "- MAE: Mean Absolut Error\n",
    "$$ \\frac{1}{n} \\sum_i |y_i - \\hat{y}_i| $$\n",
    "is the easiest to understand, because it's the average error.\n",
    "\n",
    "\n",
    "- MSE: Mean Squared Error\n",
    "$$ \\frac{1}{n} \\sum_i (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "- RMSE Rooted MSE\n",
    "$$ \\sqrt{MSE}$$\n",
    "is even more popular than MSE, because RMSE is interpretable in the \"y\" units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Estimating Coefficients\n",
    "\n",
    "The algorithm goes through different combintions of the coefficients ($ \\beta_{0} ,  \\beta_{1}  $) and picks the one with the lowest cost.\n",
    "\n",
    "$$ y = \\beta_{0}  + \\beta_{1} x $$\n",
    "\n",
    "![estimating coefficients](../img/2/coef_estimation.png)\n",
    "source: [ISLR](http://www-bcf.usc.edu/~gareth/ISL/)\n",
    "\n",
    "![gradient decent](../img/2/gradient_decent.gif)\n",
    "source: [Towards Data Science](https://towardsdatascience.com/machine-learning-fundamentals-via-linear-regression-41a5d11f5220)\n",
    "\n",
    "\n",
    "\n",
    "**Gradient Decent** is an algorithm that is used to *go down the slope* to find the minimal cost.\n",
    "When using a quadratic cost function (RSS or MSE), we can be sure there is just a single minimum.\n",
    "\n",
    "\n",
    "Linear regression is called linear, not because we are fitting a line, but because with gradient decent we can linearly progress towards the minimal cost.\n",
    "\n",
    "**R-squared**: an unit-independant meassure of model quality\n",
    "\n",
    "$$ R^2 = 1 - \\frac{\\sum_i (\\hat{y}_i - y_i)^2}{\\sum_i (\\hat{y}_i - \\mu)^2}$$\n",
    "\n",
    "$R^2 $: The coefficient of determination, pronounced \"R squared\", is the proportion of the variance in the dependent variable that is predictable from the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple linear regression attempts to model the relationship between **two or more explanatory variables** and a response variable by fitting a linear equation to observed data. \n",
    "\n",
    "$$ y = \\beta_{0}  + \\beta_{1} x_{1} + \\beta_{2} x_{2} + ... +  \\beta_{n} x_{n} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By estimating coefficients, we can find out wich of the variables influence the outcome and how.\n",
    "\n",
    "![Multiple Linear Regression](../img/2/mult_lin_regr.png)\n",
    "\n",
    "\n",
    "Caution: Variables that have been found to be important when looked at in isolation, might not be important when examined in context.\n",
    "\n",
    "<img src=\"../img/2/simpl_mult_lin_regr_comp.png\" width=\"600\">\n",
    "\n",
    "Explanation: Newspaper and Radio advertisement are not independant, but correlated. Newspaper advertisement only seems to be effective, because of its correlation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Classification\n",
    "\n",
    "- The **linear regression model** assumes that the response variable **Y is quantitative**.\n",
    "- But in many situations, the response variable is instead **qualitative or categorical**\n",
    "    - eye color ∈ {blue, brown, green}\n",
    "\t- email ∈ {ham, spam}\n",
    "- Predicting **qualitative responses** is known as **classification**. \n",
    "\n",
    "\n",
    "In this unit I will introduce three **classification methods**. \n",
    "- K-Nearest Neighbour, Logistic regression, Decision Trees, Suppor Vector Machines (SVMs)\n",
    "\n",
    "Later today you will  learn about others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Logistic regression\n",
    "Caution!!! The term \"Logistic Regression\" is a misnomer. It is NOT a regression algorithm, but a classification algorithm.\n",
    "![logistic regression](../img/2/logistic_regression.png)\n",
    "\n",
    "source: [ISLR](http://www-bcf.usc.edu/~gareth/ISL/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use:\n",
    "$$  p(x) = \\beta_{0}  + \\beta_{1} x $$\n",
    "\n",
    "    \n",
    "- Problems:\n",
    "    - negative probabilities \n",
    "    - not only values between 0 and 1\n",
    "\n",
    "- Using the **logistic function** can take care of that:\n",
    "$$  p(x) = \\frac{ e^{\\beta_{0}  + \\beta_{1} x }}{1 +  e^{\\beta_{0}  + \\beta_{1}x}} $$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 K-Nearest Neighbours (KNN)\n",
    "K Nearest Neighbors is a flexible approach to estimate the Optimal Classifier.\n",
    "For any given X we find the k closest neighbors to X in the training data, and examine their corresponding Y.\n",
    "If the majority of the Y’s are orange we predict orange otherwise guess blue.\n",
    "The smaller that k is the more flexible the method will be.\n",
    "\n",
    "<img src=\"../img/2/optimal_classifier.png\" width=\"400\">\n",
    "A simulated data set consisting of 100 observations in each of two groups, indicated in blue and in orange. The purple dashed line represents the Bayes decision boundary. The orange background grid indicates the region in which a test observation will be assigned to the orange class, and the blue background grid indicates the region in which a test observation will be assigned to the blue class.  \n",
    "\n",
    "source: [ISLR](http://www-bcf.usc.edu/~gareth/ISL/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k Nearest Neighbors is a flexible approach to estimate the Optimal Bayes Classifier.\n",
    "\n",
    "<img src=\"../img/2/knn_03.png\" width=\"500\">\n",
    "\n",
    "- For any given X we find the k closest neighbors to X in the training data, and examine their corresponding Y.\n",
    "- If the majority of the Y’s are orange we predict orange otherwise guess blue.\n",
    "- The smaller that k is the more flexible the method will be.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"../img/2/knn_10.png\" width=\"400\">\n",
    "\n",
    "\n",
    "The black curve indicates the KNN decision boundary on the data from above, using K = 10. \n",
    "The Bayes decision boundary is shown as a purple dashed line. The KNN and Bayes decision boundaries are very similar. \n",
    "\n",
    "source: [ISLR](http://www-bcf.usc.edu/~gareth/ISL/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/2/knn_1_100.png\" width=\"500\">\n",
    "\n",
    "The smaller that k is the more flexible the method will be.\n",
    "\n",
    "source: [ISLR](http://www-bcf.usc.edu/~gareth/ISL/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/2/knn_train_test_error.png\" width=\"500\">\n",
    "\n",
    "- The **training error** rates keep going down as k decreases or equivalently as the flexibility increases.\n",
    "\n",
    "- However, the **test error rate** at first decreases but then starts to increase again. \n",
    "\n",
    "\n",
    "source: [ISLR](http://www-bcf.usc.edu/~gareth/ISL/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 2.2.4 Suppor Vector Machines (SVMs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', width=300, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Exercise\n",
    "Think about a use case for machine learning in your department or domain:\n",
    "- What question do I want to answer? Typically something you do manually at the moment, or is handeled ineffectively.\n",
    "- What data set can I use to address the question?\n",
    "- Is this question really approachable by ML?\n",
    "- Is is unsupervised, supervised (regression, classification) ML?\n",
    "- What kind of algorithm would I suggest? (SVM, linear regression, clustering)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Learning Material \n",
    "- Data School Course (Kevin Markham): **Introduction to machine learning with scikit-learn** [Jupyter notebook 6](https://github.com/justmarkham/scikit-learn-videos) and [YouTube videos](https://www.youtube.com/playlist?list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A)\n",
    "- Stanford University professors Trevor Hastie and Rob Tibshirani online course based on their textbook [free PDF](http://www-bcf.usc.edu/~gareth/ISL/), **An Introduction to Statistical Learning with Applications in R (ISLR)**. [ISLR course](https://www.r-bloggers.com/in-depth-introduction-to-machine-learning-in-15-hours-of-expert-videos/)\n",
    "\n",
    "- MIT course on **Introduction to Computational Thinking and Data Science** ([lectures 9 and 10](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-0002-introduction-to-computational-thinking-and-data-science-fall-2016/lecture-videos/))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
