{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Supervised Learning\n",
    "#### Regression vs Classification\n",
    "Repetition:\n",
    "Supervised learning problems can be sub-divided into regression and classification problems.\n",
    "\n",
    "**Regression** covers situations where **y is continuous/numerical**. \n",
    "- Predicting the value of the Dow in 6 months.\n",
    "- Predicting the price of a given house based on various inputs.\n",
    "\n",
    "**Classification** covers situations where **y is categorical**\n",
    "- Will the Dow be up (U) or down (D) in 6 months?\n",
    "- Is this email a SPAM or not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Regression\n",
    "- Linear regression is a simple approach to supervised learning\n",
    "- It assumes a linear dependence of Y on $X_1, X_2, ... X_n$\n",
    "- Linear Regression is very useful (conceptually and practically)\n",
    "\n",
    "![adv](../img/2/advertise.png)\n",
    "source: [ISLR](http://www-bcf.usc.edu/~gareth/ISL/)\n",
    "\n",
    "Questions linear regression can help us to answer:\n",
    "1. Is there a relationship between advertising budget and sales? \n",
    "2. How strong is the relationship between advertising budget and sales? \n",
    "3. Which factors (media) contribute to sales?\n",
    "4. How accurately can we estimate the effect of each medium on sales? \n",
    "5. How accurately can we predict future sales?\n",
    "6. Is the relationship linear?\n",
    "7. Is there synergy among the advertising media?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Simple Linear Regression\n",
    "Simple linear regression assumes there is a linear relationship between x and y.\n",
    "$$ y = \\beta_{0}  + \\beta_{1} x $$\n",
    "\n",
    "![simple linear](../img/2/simple_linear_slope.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's gererate some random data for regression analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# generate random data from a line adding some noise\n",
    "def generate_regr(n_samples, x_mean, x_std, m, b, y_noise):\n",
    "    x, y = list(), list()\n",
    "    for i in range(n_samples):\n",
    "        x0 = random.gauss(x_mean, x_std)\n",
    "        y0 = m * x0 + b + (random.gauss(0, y_noise))\n",
    "        x.append(x0)\n",
    "        y.append(y0)\n",
    "    return np.array([x]).transpose(), y\n",
    "\n",
    "m, b, noise = 1, 0, 1\n",
    "X, y = generate_regr(50, 0, 10, m, b, noise)\n",
    "\n",
    "# plot dataset\n",
    "pyplot.close()\n",
    "pyplot.scatter(X,y)\n",
    "\n",
    "# ----------- create a model and fit it to the data ----------\n",
    "regr_model = LinearRegression()\n",
    "regr_model.fit(X, y)\n",
    "\n",
    "# regression line\n",
    "xl = np.linspace(min(X), max(X), 100)\n",
    "yl = regr_model.coef_[0] * xl + regr_model.intercept_\n",
    "yt= m * xl + b\n",
    "\n",
    "pyplot.plot(xl, yl, '-r', label='linear regression')\n",
    "pyplot.plot(xl, yt, '-b', label='truth ')\n",
    "pyplot.legend(loc='upper left')\n",
    "pyplot.show()\n",
    "\n",
    "print(\"slope:\")\n",
    "print(\"  true:\", m, \" model: \", regr_model.coef_[0])\n",
    "print(\"intercept:\")\n",
    "print(\"  true:\", b, \" model: \", regr_model.intercept_)\n",
    "\n",
    "print(\"R-squared: \", regr_model.score(X, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How does this work?\n",
    "\n",
    "The optimization algorithm tries to find a line for which the **distance of the data points (residuals)** to this line is minimized. The way to *'punish'* this distance is called a **cost function**.\n",
    "\n",
    "![residuals](../img/2/residuals.png)\n",
    "\n",
    "source: [ISLR](http://www-bcf.usc.edu/~gareth/ISL/)\n",
    "\n",
    "\n",
    "There are different **cost funtions** to pick from \n",
    "- RSS: Residual Sum of Squares\n",
    "$$  \\sum_i (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "- MAE: Mean Absolut Error\n",
    "$$ \\frac{1}{n} \\sum_i |y_i - \\hat{y}_i| $$\n",
    "is the easiest to understand, because it's the average error.\n",
    "\n",
    "\n",
    "- MSE: Mean Squared Error\n",
    "$$ \\frac{1}{n} \\sum_i (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "- RMSE Rooted MSE\n",
    "$$ \\sqrt{MSE}$$\n",
    "is even more popular than MSE, because RMSE is interpretable in the \"y\" units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Estimating Coefficients\n",
    "\n",
    "The algorithm goes through different combintions of the coefficients ($ \\beta_{0} ,  \\beta_{1}  $) and picks the one with the lowest cost.\n",
    "\n",
    "$$ y = \\beta_{0}  + \\beta_{1} x $$\n",
    "\n",
    "![estimating coefficients](../img/2/coef_estimation.png)\n",
    "source: [ISLR](http://www-bcf.usc.edu/~gareth/ISL/)\n",
    "\n",
    "![gradient decent](../img/2/gradient_decent.gif)\n",
    "source: [Towards Data Science](https://towardsdatascience.com/machine-learning-fundamentals-via-linear-regression-41a5d11f5220)\n",
    "\n",
    "\n",
    "\n",
    "**Gradient Decent** is an algorithm that is used to *go down the slope* to find the minimal cost.\n",
    "When using a quadratic cost function (RSS or MSE), we can be sure there is just a single minimum.\n",
    "\n",
    "\n",
    "Linear regression is called linear, not because we are fitting a line, but because with gradient decent we can linearly progress towards the minimal cost.\n",
    "\n",
    "**R-squared**: an unit-independant meassure of model quality\n",
    "\n",
    "$$ R^2 = 1 - \\frac{\\sum_i (\\hat{y}_i - y_i)^2}{\\sum_i (\\hat{y}_i - \\mu)^2}$$\n",
    "\n",
    "$R^2 $: The coefficient of determination, pronounced \"R squared\", is the proportion of the variance in the dependent variable that is predictable from the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple linear regression attempts to model the relationship between **two or more explanatory variables** and a response variable by fitting a linear equation to observed data. \n",
    "\n",
    "$$ y = \\beta_{0}  + \\beta_{1} x_{1} + \\beta_{2} x_{2} + ... +  \\beta_{n} x_{n} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By estimating coefficients, we can find out wich of the variables influence the outcome and how.\n",
    "\n",
    "![Multiple Linear Regression](../img/2/mult_lin_regr.png)\n",
    "\n",
    "\n",
    "Caution: Variables that have been found to be important when looked at in isolation, might not be important when examined in context.\n",
    "\n",
    "![Multiple Linear Regression](../img/2/simpl_mult_lin_regr_comp.png)\n",
    "\n",
    "Explanation: Newspaper and Radio advertisement are not independant, but correlated. Newspaper advertisement only seems to be effective, because of its correlation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Classification\n",
    "\n",
    "- The **linear regression model** assumes that the response variable **Y is quantitative**.\n",
    "- But in many situations, the response variable is instead **qualitative or categorical**\n",
    "    - eye color ∈ {blue, brown, green}\n",
    "\t- email ∈ {ham, spam}\n",
    "- Predicting **qualitative responses** is known as **classification**. \n",
    "\n",
    "\n",
    "In this unit I will introduce three **classification methods**. \n",
    "- Logistic regression, Decision Trees, Suppor Vector Machines (SVMs)\n",
    "\n",
    "Later today you will  learn about others.\n",
    "#### Logistic regression\n",
    "\n",
    "\n",
    "#### Decision Trees\n",
    "\n",
    "\n",
    "#### Suppor Vector Machines (SVMs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"300\"\n",
       "            height=\"200\"\n",
       "            src=\"http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1b21b40b7b8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', width=300, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Exercise\n",
    "Think about a use case for machine learning in your department or domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Learning Material \n",
    "- MIT course on **Introduction to Computational Thinking and Data Science** ([lectures 9 and 10](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-0002-introduction-to-computational-thinking-and-data-science-fall-2016/lecture-videos/))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
