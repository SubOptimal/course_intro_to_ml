{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language:\n",
    "\n",
    "The way we read, write, speak\n",
    "\n",
    "![img1](../img/5/img1.png)\n",
    "\n",
    "#### Understanding how language works leads to understanding how human brain works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing (NLP)\n",
    "\n",
    "- field of Computer Science  and Computational Lingustics\n",
    "- integral part of AI\n",
    "- driven by advances in AI\n",
    "\n",
    "NLP is about developing applications and services that are able to understand human languages.\n",
    "\n",
    "<img src=\"../img/5/img4.png\" alt=\"top-5-semantic-technology-trends-2017\" width=500>\n",
    "\n",
    "source: [www.ontotext.com](https://www.ontotext.com/top-5-semantic-technology-trends-2017/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making progress with NLP...\n",
    "\n",
    "- gain knowledge about the language\n",
    "- gain knowledge about the world\n",
    "- create a way to combine knowledge sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "d = {'Spam Detection' : 2,      'Text Classification' : 1,     'Named Entities Recognition' : 2,     'Information Extraction': 2,\n",
    "     'Parsing' : 1,         'Sentiment Analysis': 1,     'Speach Recognition': 2,      'Machine Translation': 2,\n",
    "    'Spell Chacking' : 1,    'Keyword Searching' : 1,     'Question Answering' : 2,     'Paraphrase' : 1,\n",
    "     'Summarization' : 1,      'Dialog' : 2,     'Chatbot': 2,     'Text Mining' : 3, \n",
    "    }\n",
    "\n",
    "my_mask = np.array(Image.open(\"../img/5/img3.png\"))\n",
    "\n",
    "#Generating wordcloud.\n",
    "#See documentation: https://github.com/amueller/word_cloud/blob/master/wordcloud/wordcloud.py\n",
    "wordcloud = WordCloud(width=2000,height=1400,  mask=my_mask, background_color=\"white\").generate_from_frequencies(d)\n",
    "\n",
    "print(\"Relevant tasks\")\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text mining / Text Analytics\n",
    "\n",
    "is the proces of deriving meaningful information from natural language text.\n",
    "\n",
    "<img src=\"../img/5/img2.png\" alt=\"?\" width=500>\n",
    "\n",
    "is this correct? [source](https://www.ontotext.com/top-5-semantic-technology-trends-2017/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textual processing\n",
    "\n",
    "- Regular Expressions\n",
    "- Word Tokenization\n",
    "- Stemming (base form or root form of words)\n",
    "- Lemmatization (maps several words into the same common root)\n",
    "- Sentence Segmentation\n",
    "\n",
    "- Part-Of-Speech Tagging\n",
    "- Chunking (picking up individual pieces of information; Groupong them into bigger pieces)\n",
    "- Dependency Parsing\n",
    "\n",
    "- Semantic Knowledge Base\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions\n",
    "\n",
    "the most basic and fundamental tool for textual processing.\n",
    "\n",
    "Variants of the same word:\n",
    "\n",
    "#### Westminster             westminster\n",
    "\n",
    "A common pattern is:\n",
    "\n",
    "#### \\[wW\\]westminster\n",
    "\n",
    "\n",
    "A pattern for any digit is:\n",
    "\n",
    "#### \\[0123456789\\]\n",
    "\n",
    "A pattern for any two digits is:\n",
    "\n",
    "#### \\[0123456789\\]\\{2\\}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of extracting ISINs from a list using RegEx\n",
    "\n",
    "import re\n",
    "\n",
    "def get_ISIN_regex(text):\n",
    "    isin = re.search(r'[A-Z]{2}[A-z0-9]{9}\\d\\b', text)\n",
    "    if isin:\n",
    "        return isin.group(0)\n",
    "\n",
    "ISIN_list = ['IE00B68JD125', 'KYG5378Q1579', 'IE000000', 'Wrong ISIN', 'Find an ISIN here: IE00B68JD125']\n",
    "\n",
    "for isin in ISIN_list:\n",
    "    print(get_ISIN_regex(isin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic text Pre-processing\n",
    "\n",
    "1. Tokenizing words in a text\n",
    "2. Normalising word formats (Stemming and Lemmatization)\n",
    "3. Segmenting senteces\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![residuals](../img/5/img5.png)\n",
    "\n",
    "Above examples must have helped you understand the concept of normalization of text, although normalization of text is not restricted to only written document but to speech as well. Stemming and Lemmatization helps us to achieve the root forms (sometimes called synonyms in search context) of inflected (derived) words.\n",
    "Stemming is different to Lemmatization in the approach it uses to produce root forms of words and the word produced.\n",
    "\n",
    "[source](https://www.datacamp.com/community/tutorials/stemming-lemmatization-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a sample text an look at keywords within it\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "text = \"\"\"The cracks in China's economy appear to be widening, with signs of weakening growth amid a background of trade tensions. Adding to the worries, China's stock market was the world's worst performer last year, ending with a loss of 28%.\n",
    "This week Apple said slowing sales in China meant it would not meet sales expectations, triggering sharp falls on global stock markets. The tech giant isn't alone. A string of other companies have issued warnings recently over China's slowdown and the impact of the trade war with the US.\n",
    "Among those are carmakers such as General Motors, Ford and Fiat Chrysler. Luxury vehicle maker Jaguar Land Rover has also warned of slowing Chinese sales.\n",
    "\"\"\"\n",
    "# [Source : https://www.bbc.com/news/business-46755158]\n",
    "\n",
    "text = \"\"\"U.S.-China trade talks to continue for third day - U.S. officials.\n",
    "BEIJING (Reuters) - The United States and China will continue trade talks in Beijing for an unscheduled third day, a member of the U.S. delegation said on Tuesday, as the world’s two largest economies looked to resolve their bitter trade dispute.\n",
    "Steven Winberg, Assistant Secretary for Fossil Energy at the U.S. Department of Energy told reporters at the U.S. delegation’s hotel that talks, which began on Monday, had gone well.\n",
    "\n",
    "“I confirm we’re continuing tomorrow, yes,” Winberg told reporters, declining to answer further questions.\n",
    "\n",
    "A spokeswoman for the U.S. Trade Representative’s office, which is leading the U.S. negotiating team, also said talks would continue on Wednesday and “a statement will likely follow then.”\n",
    "\"\"\"\n",
    "\n",
    "# [Source](https://uk.reuters.com/article/uk-usa-trade-china/u-s-china-trade-talks-to-continue-for-third-day-u-s-officials-idUKKCN1P215X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)\n",
    "\n",
    "def print_frequent_tokens(tokens):\n",
    "    freq = nltk.FreqDist(tokens)\n",
    "    for key,val in freq.items():\n",
    "        if val > 2:\n",
    "            print (str(key) + '\\t :\\t' + str(val))\n",
    "    print(\"\\nThe total number of unique keywords in the text is %s\" % len(freq))\n",
    "    \n",
    "print_frequent_tokens(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming is the process of reducing words to their word stem, base or root form.\n",
    "\n",
    "Porter Stemmer is a popular stemmer of English\n",
    "\n",
    "Examples:\n",
    "\n",
    "    weight     -> weight\n",
    "    natural    -> natur\n",
    "    language   -> languag\n",
    "    processing -> process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "tokens_stemmed = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "print_frequent_tokens(tokens_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Lemmatisation in linguistics, is the process of grouping together the different forms of a word so they can be analysed as a single item.\n",
    "\n",
    "#### How to use Lemmatizer in NLTK\n",
    "\n",
    "The NLTK Lemmatization method is based on WordNet’s built-in morphy function. Here is the introduction from WordNet official website:\n",
    "\n",
    "WordNet® is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept.\n",
    "\n",
    "[Source](https://textminingonline.com/dive-into-nltk-part-iii-part-of-speech-tagging-and-pos-tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokens_lemmatized = [wordnet_lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "print_frequent_tokens(tokens_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming or lemmatization?\n",
    "\n",
    "- Stemming and Lemmatization both generate the root form of the inflected words. The difference is that stem might not be an actual word whereas, lemma is an actual language word.\n",
    "\n",
    "- Stemming follows an algorithm with steps to perform on the words which makes it faster. Whereas, in lemmatization, you used WordNet corpus and a corpus for stop words as well to produce lemma which makes it slower than stemming. You also had to define a parts-of-speech to obtain the correct lemma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words\n",
    "\n",
    "Text may contain stop words like ‘the’, ‘is’ and ‘are’ that do not carry any significant semantic meaning. \n",
    "Stop words can be filtered from the text to be processed. \n",
    "There is no universal list of stop words in nlp research, however the nltk module contains a list of stop words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.corpus import stopwords  \n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(\"Here is a random sample of 10 words from a list of stop words in NLTK \")\n",
    "print(random.sample(stop_words, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Removing Stop words\n",
    "tokens_filtered = [w for w in tokens_lemmatized if not w in stop_words] \n",
    "        \n",
    "print_frequent_tokens(tokens_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Segmentation\n",
    "\n",
    "Sentence boundary disambiguation (SBD), also known as Sentence Breaking or Sentence Segmentation, is the problem in natural language processing of deciding where sentences begin and end. Often natural language processing tools require their input to be divided into sentences for a number of reasons. \n",
    "\n",
    "However sentence boundary identification is challenging because punctuation marks are often ambiguous. For example, a period may denote an abbreviation, decimal point, an ellipsis, or an email address – not the end of a sentence. About 47% of the periods in the Wall Street Journal corpus denote abbreviations. As well, question marks and exclamation marks may appear in embedded quotations, emoticons, computer code, and slang.\n",
    "\n",
    "[Source](https://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokenize_list = sent_tokenize(text)\n",
    "\n",
    "print(\"The original text is: \")\n",
    "print(text)\n",
    "print(\"\\nThe text split into sentences:\")\n",
    "print(sent_tokenize_list)\n",
    "print(\"\\nThe first sentence is: \")\n",
    "print(sent_tokenize_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-Of-Speech Tagging\n",
    "\n",
    "Part-of-speech tagging (POS tagging or PoS tagging or POST) is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech based on both its definition and its context.\n",
    "\n",
    "Parts of speech tagging can be important for syntactic and semantic analysis. \n",
    "\n",
    "![residuals](../img/5/img6.png)\n",
    "\n",
    "If you wish to train your own POS Tagger, go to the source link:\n",
    "[Source](https://nlpforhackers.io/training-pos-tagger/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part-Of-Speech Tagging\n",
    "\n",
    "text = \"\"\"The cracks in China's economy appear to be widening.\"\"\"\n",
    "\n",
    "print(\"Part-Of-Speech Tagging:\\n\")\n",
    "print(nltk.pos_tag(nltk.word_tokenize(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy \n",
    "\n",
    "#### Industrial-Strength Natural Language Processing\n",
    "\n",
    "spaCy is a free open-source library for Natural Language Processing in Python. It features NER, POS tagging, dependency parsing, word vectors and more.\n",
    "\n",
    "[Source](https://spacy.io/)\n",
    "(developed by Matthew Honnibal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependency Parsing\n",
    "\n",
    "text = \"\"\"Alice is Bobby's sister\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style='dep', jupyter=True)\n",
    "\n",
    "#Vizualizer available at https://explosion.ai/demos/displacy-ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Named Entity Recognition\n",
    "\n",
    "text = \"\"\"Chelsea have signed Borussia Dortmund forward Christian Pulisic for 64m euros (£58m), \n",
    "but will loan him back to the German club until the end of the season.\"\"\"\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True)\n",
    "\n",
    "#Vizualizer available at https://explosion.ai/demos/displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Practical exercise - Text Classification - Sentiments in Movie Reviews\n",
    "\n",
    "Text classification is one of the fundamental tasks in NLP with broad applications such as sentiment analysis, spam detection,  topic labeling, intent detection, etc.\n",
    "\n",
    "Spam detection software uses text classification to find out whether an incoming email should appear in the inbox or moved to the spam folder.\n",
    "\n",
    "Sentiment analysis is a type of text classification whose goal is to determine the polarity of content, in particular, the opinion of the author. Sentiment analysis is useful to analyze Twitter posts to find out people's opinion of a movie, or to gather people’s opinion about a new product.\n",
    "\n",
    "### Text Classification Workflow\n",
    "\n",
    "![image](../img/5/img7.png)\n",
    "\n",
    "[Source](https://developers.google.com/machine-learning/guides/text-classification/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task description\n",
    "\n",
    "We will train a text classifer for sentiment analysis using the Internet Movie Database (IMDb) movie reviews dataset.\n",
    "This dataset consists of reviews posted by people on the IMDb website and corresponding labels (“positive” or “negative”) which indicate the reviewer's opinion about the movie.\n",
    "\n",
    "[Dataset] (http://ai.stanford.edu/~amaas/data/sentiment/) is publicly available.\n",
    "\n",
    "The first step is to gather data. \n",
    "The more training examples you have the better you are able to train the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb_sentiment_analysis_dataset(data_path, seed=123):\n",
    "    \"\"\"Loads the IMDb movie reviews sentiment analysis dataset.\n",
    "\n",
    "    # Arguments\n",
    "        data_path: string, path to the data directory.\n",
    "        seed: int, seed for randomizer.\n",
    "\n",
    "    # Returns\n",
    "        A tuple of training and validation data.\n",
    "        Number of training samples: 25000\n",
    "        Number of test samples: 25000\n",
    "        Number of categories: 2 (0 - negative, 1 - positive)\n",
    "\n",
    "    # References\n",
    "        Mass et al., http://www.aclweb.org/anthology/P11-1015\n",
    "\n",
    "        Download and uncompress archive from:\n",
    "        http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    \"\"\"\n",
    "    imdb_data_path = os.path.join(data_path, 'aclImdb')\n",
    "    \n",
    "    imdb_data_path = data_path\n",
    "\n",
    "    # Load the training data\n",
    "    train_texts = []\n",
    "    train_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        train_path = os.path.join(data_path, 'train', category)\n",
    "        for fname in sorted(os.listdir(train_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(train_path, fname), encoding=\"utf8\") as f:\n",
    "                    train_texts.append(f.read())\n",
    "                train_labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "    # Load the validation data.\n",
    "    test_texts = []\n",
    "    test_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        test_path = os.path.join(data_path, 'test', category)\n",
    "        for fname in sorted(os.listdir(test_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(test_path, fname), encoding=\"utf8\") as f:\n",
    "                    test_texts.append(f.read())\n",
    "                test_labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "    # Shuffle the training data and labels.\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_texts)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_labels)\n",
    "\n",
    "    return train_texts, train_labels, test_texts, test_labels\n",
    "\n",
    "#Source https://developers.google.com/machine-learning/guides/text-classification/step-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "\n",
    "imdb_data_path = \"../datasets/aclImdb_v1/aclImdb/\"\n",
    "\n",
    "train_texts, train_labels, test_texts, test_labels = load_imdb_sentiment_analysis_dataset(imdb_data_path, 123)\n",
    "\n",
    "data = ((train_texts, np.array(train_labels)),\n",
    "        (test_texts, np.array(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = 10\n",
    "\n",
    "print(train_texts[item])\n",
    "print(\"\\nLabel is %s\" % \"Positive\" if train_labels[item] == 1 else \"Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_num_words_per_sample(sample_texts):\n",
    "    \"\"\"Returns the median number of words per sample given corpus.\n",
    "\n",
    "    # Arguments\n",
    "        sample_texts: list, sample texts.\n",
    "\n",
    "    # Returns\n",
    "        int, median number of words per sample.\n",
    "    \"\"\"\n",
    "    num_words = [len(s.split()) for s in sample_texts]\n",
    "    return np.median(num_words)\n",
    "\n",
    "def plot_sample_length_distribution(sample_texts):\n",
    "    \"\"\"Plots the sample length distribution.\n",
    "\n",
    "    # Arguments\n",
    "        samples_texts: list, sample texts.\n",
    "    \"\"\"\n",
    "    plt.hist([len(s) for s in sample_texts], 50)\n",
    "    plt.xlabel('Length of a sample')\n",
    "    plt.ylabel('Number of samples')\n",
    "    plt.title('Sample length distribution')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def plot_frequency_distribution_of_ngrams(sample_texts,\n",
    "                                          ngram_range=(1, 2),\n",
    "                                          num_ngrams=50):\n",
    "    \"\"\"Plots the frequency distribution of n-grams.\n",
    "    # Arguments\n",
    "        samples_texts: list, sample texts.\n",
    "        ngram_range: tuple (min, mplt), The range of n-gram values to consider.\n",
    "            Min and mplt are the lower and upper bound values for the range.\n",
    "        num_ngrams: int, number of n-grams to plot.\n",
    "            Top `num_ngrams` frequent n-grams will be plotted.\n",
    "    \"\"\"\n",
    "    # Create args required for vectorizing.\n",
    "    kwargs = {\n",
    "            'ngram_range': (1, 1),\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': 'word',  # Split text into word tokens.\n",
    "    }\n",
    "    vectorizer = CountVectorizer(**kwargs)\n",
    "\n",
    "    # This creates a vocabulary (dict, where keys are n-grams and values are\n",
    "    # idxices). This also converts every text to an array the length of\n",
    "    # vocabulary, where every element idxicates the count of the n-gram\n",
    "    # corresponding at that idxex in vocabulary.\n",
    "    vectorized_texts = vectorizer.fit_transform(sample_texts)\n",
    "\n",
    "    # This is the list of all n-grams in the index order from the vocabulary.\n",
    "    all_ngrams = list(vectorizer.get_feature_names())\n",
    "    num_ngrams = min(num_ngrams, len(all_ngrams))\n",
    "    # ngrams = all_ngrams[:num_ngrams]\n",
    "\n",
    "    # Add up the counts per n-gram ie. column-wise\n",
    "    all_counts = vectorized_texts.sum(axis=0).tolist()[0]\n",
    "\n",
    "    # Sort n-grams and counts by frequency and get top `num_ngrams` ngrams.\n",
    "    all_counts, all_ngrams = zip(*[(c, n) for c, n in sorted(\n",
    "        zip(all_counts, all_ngrams), reverse=True)])\n",
    "    ngrams = list(all_ngrams)[:num_ngrams]\n",
    "    counts = list(all_counts)[:num_ngrams]\n",
    "    \n",
    "    plt.figure(figsize=(15,8))\n",
    "    idx = np.arange(num_ngrams)\n",
    "    plt.bar(idx, counts, width=0.8, color='b')    \n",
    "    plt.xlabel('N-grams')\n",
    "    plt.ylabel('Frequencies')\n",
    "    plt.title('Frequency distribution of n-grams')\n",
    "    plt.xticks(idx, ngrams, rotation=45)    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Exploring Training set\")\n",
    "print(\"Median number of words per sample given corpus is %i\" % get_num_words_per_sample(train_texts))\n",
    "plot_sample_length_distribution(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Exploring Testing set\")\n",
    "print(\"Median number of words per sample given corpus is %i\" % get_num_words_per_sample(test_texts))\n",
    "plot_sample_length_distribution(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frequency_distribution_of_ngrams(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frequency_distribution_of_ngrams(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "def plot_wordcloud(text):\n",
    "    \"\"\"Generates wordclouds from text.\n",
    "    # Arguments\n",
    "        text: string, sample text.\n",
    "    \"\"\"    \n",
    "    # Generate a word cloud image with a limited max_font_size\n",
    "    wordcloud = WordCloud(max_font_size=40, background_color=\"white\").generate(text)\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Word Clouds from the Training Set\")\n",
    "\n",
    "joined_train_text = ' '.join(train_texts)\n",
    "plot_wordcloud(joined_train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Word Clouds from the Testing Set\")\n",
    "\n",
    "joined_test_text = ' '.join(test_texts)\n",
    "plot_wordcloud(joined_test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# Tokenization and Vectorization\n",
    "# Vectorization parameters\n",
    "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
    "NGRAM_RANGE = (1, 2)\n",
    "# Whether text should be split into word or character n-grams.\n",
    "# One of 'word', 'char'.\n",
    "TOKEN_MODE = 'word'\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "MIN_DOCUMENT_FREQUENCY = 5\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 10000\n",
    "# Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "kwargs = {\n",
    "        'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
    "        'dtype': 'int32',\n",
    "        'strip_accents': 'unicode',\n",
    "        'decode_error': 'replace',\n",
    "        'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "        'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Tokenizing samples into unigrams + bigrams provides good accuracy while taking less compute time.\n",
    "# We use Tf-idf encoding for vectorization. This usually performs better than one-hot encoding and count encoding in \n",
    "# terms of accuracy. Tf-idf uses floating point representation and takes more time to compute and uses more memory.\n",
    "vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "# Learn vocabulary from training texts and vectorize training texts.\n",
    "x_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "# Vectorize testing texts.\n",
    "x_test = vectorizer.transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# When we convert texts to tokens we may end up with a large number of tokens. We want to drop rarely occurring tokens\n",
    "# as well as tokens that don't contribute heavily to label predictions. \n",
    "# We use the `chi2` function to identify the top 10K features.\n",
    "# Select top 'K' of the vectorized features.\n",
    "selector = SelectKBest(chi2, k=min(TOP_K, x_train.shape[1]))\n",
    "selector.fit(x_train, train_labels)\n",
    "x_train = selector.transform(x_train)\n",
    "x_test = selector.transform(x_test)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and Training the Model; Tuning hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'C': np.logspace(-2, 10, base=2, num=4)}\n",
    "logreg = LogisticRegression()\n",
    "clf = GridSearchCV(logreg, parameters, cv=4)\n",
    "clf.fit(x_train, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best score: %s\" % clf.best_score_)\n",
    "print(\"Best parameters:\")\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "pred_test = clf.predict(x_test)\n",
    "\n",
    "print(\"The accuracy is %s\" % str(round(100*metrics.accuracy_score(pred_test, test_labels), 2)))\n",
    "print(\"The precision is %s\" % str(round(100*metrics.precision_score(pred_test, test_labels), 2)))\n",
    "print(\"The recall    is %s\" % str(round(100*metrics.recall_score(pred_test, test_labels), 2)))\n",
    "print(\"The F1 score  is %s\" % str(round(100*metrics.f1_score(pred_test, test_labels), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The confusion matrix:\\n\")\n",
    "print(metrics.confusion_matrix(pred_test, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report: \\n\")\n",
    "print(metrics.classification_report(pred_test, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the model\n",
    "\n",
    "We save the model into a pickle file.\n",
    "This file can be easily loaded into a separate application for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "#Saving the model\n",
    "modelname = 'sentiment_classifier.pkl'\n",
    "joblib.dump(clf, os.path.join('../output/', modelname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "In this tutorial, we have done the following:\n",
    "\n",
    "1. Reviewed the concepts of Natural Language Preprocessing\n",
    "2. Discussed applications of NLP for solving real life problems\n",
    "3. Examined textual pre-processing tasks\n",
    "4. Run a practical exercise of sentiment classification\n",
    "5. Explored dataset of movies reviews\n",
    "6. Trained, tested and evaluated a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
