{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Model Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In machine learning, model evaluation is referred to as the process where a trained model is\n",
    " evaluated with a testing data set. The testing data set is a separate portion of the same data\n",
    " set from which the training set is derived. The main purpose of using the testing data set is to\n",
    " test the generalization ability of a trained model ([Alpaydin 2010](http://scholar.google.com/scholar_lookup?title=Introduction%20to%20machine%20learning&author=E.%20Alpaydin&publication_year=2010)).\n",
    " \n",
    " The outline of the model building and evaluation process is given below.\n",
    " <img src=\"../img/4/img1.png\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, TimeSeriesSplit\n",
    "from sklearn.metrics import average_precision_score, accuracy_score, precision_recall_curve\n",
    "\n",
    "# data are stored here\n",
    "data_folder = '../datasets'\n",
    "\n",
    "# integer for the random seed\n",
    "THE_ANSWER = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = np.load('%s/data.npz' % data_folder)\n",
    "\n",
    "# extract training data\n",
    "X_train, y_train = data['X_train'], data['y_train']\n",
    "# Extract testing data\n",
    "X_test, y_test = data['X_test'], data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training / Testing dataset\n",
    "\n",
    "The most important thing you can do to properly evaluate your model is to not train the model on the entire dataset.\n",
    "\n",
    "You can easily split the data into training and testing sub-sets of data using a standard train_test_split function from sklearn:\n",
    "\n",
    "#### sklearn.model_selection.train_test_split(*arrays, **options)\n",
    "\n",
    "In our dataset, the data is already divided into the training and testing subsets.\n",
    "\n",
    "[Source](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "![train/test](../img/4/img2.png)\n",
    "\n",
    "[source](https://www.experfy.com/blog/train-test-split-and-cross-validation-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first datapoint contains the following features: \n",
      "\n",
      "\n",
      "[ 0.6156176  -0.78584207  0.56566955  0.60753437 -0.17992427 -0.32535287\n",
      " -0.32719857 -0.32719857 -0.15201037  0.18929607  0.21877548         nan\n",
      " -1.564559           nan  1.87409565  0.3909332  -1.64262716 -0.33748312\n",
      "  1.64262716         nan         nan  0.90230299         nan         nan\n",
      "  1.37062409 -0.00604467 -0.01317495 -0.76401196  0.93544524 -0.04058151\n",
      " -0.17166864 -0.02962516 -0.26744559]\n",
      "\n",
      "\n",
      "The number of data points in the training dataset is 65579\n",
      "\n",
      "\n",
      "The number of data points in the testing dataset is  22034\n"
     ]
    }
   ],
   "source": [
    "#Let's have a look at the first datapoint\n",
    "print(\"The first datapoint contains the following features: \")\n",
    "print(\"\\n\")\n",
    "print(X_train[0])\n",
    "print(\"\\n\")\n",
    "print(\"The number of data points in the training dataset is \" + str(y_train.size))\n",
    "print(\"\\n\")\n",
    "print(\"The number of data points in the testing dataset is  \" + str(y_test.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 1. ... 0. 0. 0.]\n",
      "The class == 1 rows make 34% of the data set.\n"
     ]
    }
   ],
   "source": [
    "# let's take a look at the class labels stored in the y_train array\n",
    "print(y_train)\n",
    "print('The class == 1 rows make %0.f%% of the data set.' % (100*y_train.sum()/y_train.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only 34% of rows are of class 1 --> the data set is imbalanced.\n",
    "\n",
    "Let's start with training a classifier on the training set and then testing its performance on the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# print out the default values of model hyper-parameters (will talk about hyper-parameters later)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=None,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the number of trees to 500, and the random seed to THE_ANSWER for reproducibility\n",
    "model.set_params(n_estimators=500, random_state=THE_ANSWER)\n",
    "#model = RandomForestClassifier(n_estimators=500, random_state=THE_ANSWER) # another way of doing it\n",
    "\n",
    "# check that the model changed\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some values in the training set are NaN (i.e., are missing).\n",
    "# The Random Forest classifier does not like that, so we set NaN values to 10000.\n",
    "X_train_fixed = X_train.copy()\n",
    "X_train_fixed[np.isnan(X_train_fixed)] = 10000\n",
    "\n",
    "#Repeat the same transformation for the testing dataset\n",
    "# first, if there are NaNs in the test set, set them to 10000.\n",
    "X_test_fixed = X_test.copy()\n",
    "X_test_fixed[np.isnan(X_test)] = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 53.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=None,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# fit the model using training data\n",
    "model.fit(X_train_fixed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have fit the model, we need to evaluate its performance.\n",
    "That can be done using various metrics, but the simplest one is accuracy. Accuracy is defined as the percentage of correct predictions for the test data.\n",
    "\n",
    "### accuracy = number of correct predictions / number of all predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: [0. 0. 0. ... 1. 0. 0.]\n",
      "True class:      [1. 0. 0. ... 1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# push the test data set through the model to get the predicted classes\n",
    "y_scores = model.predict(X_test_fixed)\n",
    "\n",
    "# the model predicts the class.\n",
    "print('Predicted class:', y_scores)\n",
    "print('True class:     ', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.892.\n"
     ]
    }
   ],
   "source": [
    "# calculate the accuracy\n",
    "print('The accuracy is %.3f.' % accuracy_score(y_test, y_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great result, right? May not be since we have an imbalanced data set.\n",
    "Here's an extreme example: Imagine a data set where 90% of rows are of class 0.\n",
    "A naive classifier that predicts 0 for all rows will have an accuracy of 90%!\n",
    "\n",
    "\n",
    "What most business people would like to know is the following:\n",
    "\n",
    "#### 1. What is the detection rate? Did we correctly identify 50%, 80%, or more of all class 1 objects (e.g., instructions that will fail to settle)?\n",
    "#### 2. What is the false alarm rate? What percentage of those tagged as class 1 are *not* class 1?\n",
    "\n",
    "The detection rate is called \"recall\" in machine learning,\n",
    "and the false alarm rate is related to \"precision\" (actually, it is 1-precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENIA: use graphics on this page to explain the confusion matrix, precision, and recall\n",
    "# https://www.jeremyjordan.me/evaluating-a-machine-learning-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Classification metrics\n",
    "When performing classification predictions, there's four types of outcomes that could occur.\n",
    "\n",
    "#### True positives\n",
    "are when you predict an observation belongs to a class and it actually does belong to that class.\n",
    "#### True negatives\n",
    "are when you predict an observation does not belong to a class and it actually does not belong to that class.\n",
    "#### False positives\n",
    "occur when you predict an observation belongs to a class when in reality it does not.\n",
    "#### False negatives\n",
    "occur when you predict an observation does not belong to a class when in fact it does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt    \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "labels = [1, 0]\n",
    "cm = confusion_matrix(y_test, y_scores)\n",
    "\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=True, ax = ax, fmt=' ', cmap=\"Blues\"); #annot=True to annotate cells\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); \n",
    "#ax.xaxis.set_ticklabels([1, 0]); ax.yaxis.set_ticklabels([1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True  Positives = 6469\n",
      "True  Negatives = 13194\n",
      "False Positives = 1031\n",
      "False Negatives = 1340\n"
     ]
    }
   ],
   "source": [
    "TP = cm[1][1]\n",
    "TN = cm[0][0]\n",
    "FP = cm[0][1]\n",
    "FN = cm[1][0]\n",
    "\n",
    "print(\"True  Positives = %i\" % TP)\n",
    "print(\"True  Negatives = %i\" % TN)\n",
    "print(\"False Positives = %i\" % FP)\n",
    "print(\"False Negatives = %i\" % FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "is defined as the percentage of correct predictions. It can be calculated easily by dividing the number of correct predictions by the number of total predictions.\n",
    "\n",
    "![image](../img/4/img3.png)\n",
    "\n",
    "### Precision \n",
    "is defined as the fraction of relevant examples (true positives) among all of the examples which were predicted to belong in a certain class.\n",
    "\n",
    "![image](../img/4/img4.png)\n",
    "\n",
    "### Recall \n",
    "is defined as the fraction of examples which were predicted to belong to a class with respect to all of the examples that truly belong in the class.\n",
    "\n",
    "![image](../img/4/img5.png)\n",
    "\n",
    "[Source](https://www.jeremyjordan.me/evaluating-a-machine-learning-model/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](../img/4/img7.png)\n",
    "\n",
    "[Source](https://www.jeremyjordan.me/evaluating-a-machine-learning-model/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision and recall are useful in cases where classes aren't evenly distributed. The common example is for developing a classification algorithm that predicts whether or not someone has a disease. If only a small percentage of the population (let's say 1%) has this disease, we could build a classifier that always predicts that the person does not have the disease, we would have built a model which is 99% accurate and 0% useful. However, if we measured the recall of this useless predictor, it would be clear that there was something wrong with our model.\n",
    "\n",
    "A common approach for combining the precision and recall metrics is known as the f-score:\n",
    "\n",
    "![image](../img/4/img8.png)\n",
    "\n",
    "where beta is often set to 1\n",
    "\n",
    "![image](../img/4/img10.png)\n",
    "\n",
    "[Source](https://www.jeremyjordan.me/evaluating-a-machine-learning-model/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy  is 89.24\n",
      "The precision is 86.25\n",
      "The recall    is 82.84\n",
      "The F1        is 84.51\n"
     ]
    }
   ],
   "source": [
    "#Let's look at accuracy, precision and recall of our trained classifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_scores)\n",
    "print(\"The accuracy  is %.2f\" % np.round(100*accuracy, 2))\n",
    "\n",
    "precision = precision_score(y_test, y_scores)\n",
    "print(\"The precision is %.2f\" % np.round(100*precision, 2))\n",
    "\n",
    "recall = recall_score(y_test, y_scores)\n",
    "print(\"The recall    is %.2f\" % np.round(100*recall, 2))\n",
    "\n",
    "f1 = f1_score(y_test, y_scores)\n",
    "print(\"The F1        is %.2f\" % np.round(100*f1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.88866667 0.11133333]\n",
      " [0.828      0.172     ]\n",
      " [0.85693333 0.14306667]\n",
      " ...\n",
      " [0.378      0.622     ]\n",
      " [0.584      0.416     ]\n",
      " [0.804      0.196     ]]\n",
      "The y_scores is a 2-dimensional array: (22034, 2)\n"
     ]
    }
   ],
   "source": [
    "# push the test data set through the model to get the classification *scores* (not labels)\n",
    "# (note that we use the \"predict_proba\" method and not \"predict\")\n",
    "y_proba = model.predict_proba(X_test_fixed)\n",
    "\n",
    "# the model calculates the score for both classes (0 and 1). Each row sums to 1.\n",
    "print(y_proba)\n",
    "print('The y_scores is a 2-dimensional array:', y_proba.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under the Precision-Recall curve is 0.923.\n"
     ]
    }
   ],
   "source": [
    "# calculate the area under the precision-recall curve for the \"positive\" class\n",
    "# by feeding the true class label (y_test) and the predicted classification scores (y_scores[:, 1])\n",
    "print('The area under the Precision-Recall curve is %.3f.' % average_precision_score(y_test, y_proba[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different metrics for evaluating classification models can be found here:\n",
    "# https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, y_valid = data['X_valid'], data['y_valid']\n",
    "X_devel, y_devel = data['X_devel'], data['y_devel']\n",
    "X_test, y_test = data['X_test'], data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
