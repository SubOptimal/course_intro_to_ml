{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [4. Model evaluation](#4)\n",
    "    - [4.1 Building a classifier model using the Random Forest algorithm](#4_1)\n",
    "    - [4.2 Evaluating a model using classification metrics](#4_2)\n",
    "        - [4.2.2 Accuracy](#4_2_2)\n",
    "        - [4.2.3 Precision and Recall](#4_2_3)\n",
    "        - [4.2.4 Log loss or cross-entropy](#4_2_4)\n",
    "    - [4.3 Regression metrics](#4_3)\n",
    "    - [4.4 Strategies for splitting data into training, validation, and test sets](#4_4)\n",
    "        - [4.4.1 Hold-out (Out-of-Sample) validation](#4_4_1)\n",
    "        - [4.4.2 k-fold cross-validation (CV)](#4_4_2)\n",
    "        - [4.4.3 Why shuffling and cross-validation are very, very bad when using time-dependent data (e.g., financial data)](#4_4_3)\n",
    "    - [Recap: What we have learned so far?](#recap)\n",
    "    - [4.5 Optimization of model hyper-parameters](#4_5)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook you will find how to:\n",
    "\n",
    "1. Build a classifier,\n",
    "2. Evaluate its performance,\n",
    "3. Further optimize it, and\n",
    "4. Avoid some nasty mistakes in the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model evaluation<a name=\"4\"></a>\n",
    "In machine learning, model validation is referred to as the process where a trained model is evaluated with a testing data set. The testing data set is a separate portion of the same data set from which the training set is derived. The main purpose of using the testing data set is to test the generalization ability of a trained model ([Alpaydin 2010](http://scholar.google.com/scholar_lookup?title=Introduction%20to%20machine%20learning&author=E.%20Alpaydin&publication_year=2010)).\n",
    " \n",
    " In simpler terms, model evaluation tells you what kind of performance you can expect when your run new data through the model.\n",
    " \n",
    " The outline of the model building and evaluation process is given below.\n",
    " <img src=\"../img/4/model_evaluation_process.png\" width=\"400\">\n",
    " \n",
    " We will start by training a classification model that uses the [scikit-learn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) of the [Random Forest](https://en.wikipedia.org/wiki/Random_forest) algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Building a classifier model using the Random Forest algorithm<a name=\"4_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load necessary Python packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import randint as sp_randint, uniform as sp_uniform\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, TimeSeriesSplit\n",
    "from sklearn.metrics import average_precision_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve, precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, RandomizedSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# data are stored here\n",
    "data_folder = '../datasets'\n",
    "\n",
    "# integer for the random seed\n",
    "THE_ANSWER = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all data\n",
    "data = np.load('%s/data.npz' % data_folder)\n",
    "\n",
    "# extract training data\n",
    "# (strategies for splitting data into training and test sets are detailed in Section 4.4)\n",
    "X_train, y_train = data['X_train'], data['y_train']\n",
    "\n",
    "#Let's have a look at the first datapoint\n",
    "print(\"The first datapoint contains the following features:\\n\")\n",
    "print(X_train[0], '\\n')\n",
    "print(\"Each data point is described by %d features.\" % X_train.shape[1])\n",
    "print(\"The number of data points in the training dataset is %d.\" % X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at the class labels stored in the y_train array\n",
    "print(y_train)\n",
    "print('The class == 1 objects make %0.f%% of the training set.' % (100*y_train.sum()/y_train.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**34% of objects are of class 1 &#9658; the training set is imbalanced**\n",
    "\n",
    "Let's start with training a classifier on the training set and then testing its performance on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# print out the default values of model hyper-parameters (will talk about hyper-parameters later)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number of trees to 500, and the random seed to THE_ANSWER for reproducibility\n",
    "model.set_params(n_estimators=500, random_state=THE_ANSWER)\n",
    "#model = RandomForestClassifier(n_estimators=500, random_state=THE_ANSWER) # another way of doing it\n",
    "\n",
    "# check that the model changed\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some values in the training set are NaN (i.e., are missing).\n",
    "# The Random Forest classifier does not like that, so we set NaN values to 10000.\n",
    "X_train_fixed = X_train.copy()\n",
    "X_train_fixed[np.isnan(X_train_fixed)] = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model using training data\n",
    "model.fit(X_train_fixed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Evaluating a model using classification metrics<a name=\"4_2\"></a>\n",
    "\n",
    "Now that we have fit a model, we need to evaluate its performance on a test set. Strategies for splitting data into training and test sets are detailed in [Section 4.4](#4_4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, extract test data\n",
    "X_test, y_test = data['X_test'], data['y_test']\n",
    "\n",
    "# if there are NaNs in the test set, set them to 10000.\n",
    "X_test_fixed = X_test.copy()\n",
    "X_test_fixed[np.isnan(X_test)] = 10000\n",
    "\n",
    "# sanity check: only 35% of objects are of class 1 --> the test set is also imbalanced\n",
    "#print('The class == 1 objects make %0.f%% of the test set.\\n' % (100*y_test.sum()/y_test.size))\n",
    "\n",
    "# now push the test set through the model to get the predicted classes\n",
    "y_pred = model.predict(X_test_fixed)\n",
    "\n",
    "print('True class:     ', y_test[0:20])\n",
    "print('Predicted class:', y_pred[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Confusion matrix<a name=\"4_2_1\"></a>\n",
    "When performing classification predictions, there are four types of outcomes that can occur.\n",
    "\n",
    "#### True positives\n",
    "are when you predict an observation belongs to a class and it actually does belong to that class.\n",
    "#### True negatives\n",
    "are when you predict an observation does not belong to a class and it actually does not belong to that class.\n",
    "#### False positives\n",
    "occur when you predict an observation belongs to a class when in reality it does not.\n",
    "#### False negatives\n",
    "occur when you predict an observation does not belong to a class when in fact it does.\n",
    "\n",
    "These four outcomes are often plotted on a confusion matrix. Let's plot the confusion matrix for the predictions we have obtained above on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [1, 0]\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=True, ax = ax, fmt=' ', cmap=\"Blues\"); #annot=True to annotate cells\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); \n",
    "#ax.xaxis.set_ticklabels([1, 0]); ax.yaxis.set_ticklabels([1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = cm[1][1]\n",
    "TN = cm[0][0]\n",
    "FP = cm[0][1]\n",
    "FN = cm[1][0]\n",
    "\n",
    "print(\"True  Positives = %i\" % TP)\n",
    "print(\"True  Negatives = %i\" % TN)\n",
    "print(\"False Positives = %i\" % FP)\n",
    "print(\"False Negatives = %i\" % FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having these numbers, we can now calculate some classification metrics, such as accuracy, precision, and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Accuracy<a name=\"4_2_2\"></a>\n",
    "\n",
    "Accuracy is defined as the percentage of correct predictions for the test data.\n",
    "\n",
    "$$accuracy = \\frac{{\\rm number\\, of\\, correct\\, predictions}}{{\\rm number\\, of\\, all\\, predictions}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the accuracy\n",
    "print('The accuracy is %.3f.' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a great result, right? Maybe not since we have an imbalanced data set (i.e., class == 1 objects make only 35% of the test set). Here's an extreme example how accuracy can be misleading when the classes are imbalanced:\n",
    "\n",
    "Imagine a data set where 90% of objects are of class 0. A naive classifier that predicts 0 for all objects will have an accuracy of 90%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Precision and Recall<a name=\"4_2_3\"></a>\n",
    "\n",
    "In my opinion, what most business people would like to know is the following:\n",
    "1. What is the detection rate? Did we correctly identify 50%, 80%, or more of all class 1 objects (e.g., instructions that will fail to settle)?\n",
    "2. What is the false alarm rate? What percentage of those tagged as class 1 are **not** of class 1?\n",
    "\n",
    "The detection rate is called [\"recall\"](https://en.wikipedia.org/wiki/Precision_and_recall) in machine learning, and the false alarm rate is related to [\"precision\"](https://en.wikipedia.org/wiki/Precision_and_recall) (actually, it is 1-precision).\n",
    "\n",
    "#### Precision\n",
    "is defined as the fraction of relevant examples (true positives) among all of the examples which were predicted to belong in a certain class.\n",
    "$$Precision = \\frac{true\\, positives}{true\\, positives+false\\, positives}$$\n",
    "\n",
    "#### Recall\n",
    "is defined as the fraction of examples which were predicted to belong to a class with respect to all of the examples that truly belong in the class.\n",
    "$$Recall = \\frac{true\\, positives}{true\\, positives+false\\, negatives}$$\n",
    "\n",
    "![image](../img/4/img7.png)\n",
    "\n",
    "[Source](https://www.jeremyjordan.me/evaluating-a-machine-learning-model/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate precision and recall using the predictions we have obtained above on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = precision_score(y_test, y_pred)\n",
    "r = recall_score(y_test, y_pred)\n",
    "print('The precision obtained on the test set is %.0f%% and recall is %.0f%%.' % (p*100, r*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above precision and recall indicate that our model is able to detect 83% of all class == 1 objects (e.g., failed settlements), while having a false alarm rate of only 14% (remember that the false alarm rate = 1 - precision).\n",
    "\n",
    "But what if a detection rate of 83% is not good enough for your use case? What if you willing to accept a higher false alarm rate if that would enable you to identify more of class == 1 objects (i.e., have a higher detection rate)?\n",
    "\n",
    "This can be achieved by:\n",
    "1. Calculating classification **scores** (values 0 *to* 1) instead of predicting class **labels** (values 0 *or* 1),\n",
    "2. Plotting the precision-recall curve, and\n",
    "3. Selecting all objects that have the classification score higher than some threshold.\n",
    "\n",
    "Let's start by calculating and explaining classification scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push the test data set through the model to get the classification *scores* (not labels)\n",
    "# (note that we use the \"predict_proba\" method and not \"predict\")\n",
    "y_scores = model.predict_proba(X_test_fixed)\n",
    "\n",
    "# the model calculates the score for both classes (0 and 1). Each row sums to 1.\n",
    "print(y_scores[0:5, :])\n",
    "print('The y_scores is a 2-dimensional array:', y_scores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have classification scores, let's plot the (1-precision) vs. recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the detection rate vs. false alarm rate\n",
    "p, r, thresh = precision_recall_curve(y_test, y_scores[:, 1])\n",
    "thresh = np.append(0, thresh)\n",
    "print('Score > %.3f:  %.0f%% detection rate at 10%% false alarm rate.' % (thresh[(1-p) > 0.1][-1], 100*r[(1-p) > 0.1][-1]))\n",
    "print('Score > %.3f:  %.0f%% detection rate at 15%% false alarm rate.' % (thresh[(1-p) > 0.15][-1], 100*r[(1-p) > 0.15][-1]))\n",
    "print('Score > %.3f:  %.0f%% detection rate at 20%% false alarm rate.' % (thresh[(1-p) > 0.2][-1], 100*r[(1-p) > 0.2][-1]))\n",
    "print('Score > 0.000: 100%% detection rate at %.0f%% false alarm rate (naive approach).' % (100-100*y_test.sum()/y_test.size))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.step((1-p)*100, r*100, color='b', where='post', label='late settlements')\n",
    "ax.set_xlabel('False alarm rate [%]')\n",
    "ax.set_ylabel('Detection rate [%]')\n",
    "ax.set_ylim([0.0, 101])\n",
    "_ = ax.set_xlim([0.0, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, if we select all objects (rows) that have the classification score (y_scores[:, 1]) greater than 0.557, we can expect a sample where 78% of objects will be of class 1 (e.g., will be failed settlements), and only 10% will be of class 0.\n",
    "\n",
    "**Example**: Let's say there are 100 objects (rows) that have the classification score greater than 0.577. This mean that 90 objects will truly be of class 1 (false alarm rate is 10% &#9658; precision is 90% &#9658; 100*0.9 = 90). The total number of class 1 objects is 115 ( = 90/0.78 &#9658; because detection rate is 78%), which means the algorithm missed 25 objects (rows) of class 1 (i.e., it tagged them as class == 0, instead of class == 1).\n",
    "\n",
    "The area under the precision-recall curve (AUPRC) or [average precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html) is also a classification metric. The perfect classifier (model) has a detection rate of 100% and false alarm rate of 0%, and the AUPRC = 1.\n",
    "\n",
    "What is the AUPRC for the model we trained above? Let's calculate it using the test data set.<a name=\"precision_recal_sec_end\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the average precision (AUPRC) for class == 1 objects\n",
    "# by feeding the true class label (y_test) and the predicted classification scores (y_scores[:, 1])\n",
    "print('The area under the precision-recall curve is %.3f.' % average_precision_score(y_test, y_scores[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What advantage does the area under the precision-recall curve (AUPRC) have over the precision and recall?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 Log loss or cross-entropy<a name=\"4_2_4\"></a>\n",
    "\n",
    "The metric of choice for evaluating multi-class classification models. For more details, see [here](http://wiki.fast.ai/index.php/Log_Loss) and [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html#sklearn.metrics.log_loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Regression metrics<a name=\"4_3\"></a>\n",
    "\n",
    "**(Root) Mean Squared Error** is simply defined as the (root of the) average of squared differences between the predicted output and the true output. A smaller value of this metric indicates a better model. The root of MSE is in units of *y*.\n",
    "$$MSE(y_{true}, y_{pred})=\\frac{1}{n_{samples}}\\sum{(y_{true} - y_{pred})^2}$$\n",
    "\n",
    "[${\\bf R^2}$ **coefficient**](https://en.wikipedia.org/wiki/Coefficient_of_determination) or the **coefficient of determination** represents the proportion of variance in the outcome that our model is capable of predicting based on its features. The values range from 1 (perfect model) to minus infinity (the model can be infinitely bad).\n",
    "$$R^2(y_{true}, y_{pred}) = 1 - \\frac{\\sum{(y_{true} - y_{pred})^2}}{\\sum{(y_{true} - \\bar{y})^2}}; \\bar{y} = \\frac{1}{n_{samples}}\\sum{y_{true}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Strategies for splitting data into training, validation, and test sets<a name=\"4_4\"></a>\n",
    "\n",
    "### 4.4.1 Hold-out (Out-of-Sample) validation<a name=\"4_4_1\"></a>\n",
    "<img src=\"../img/4/hold-out_validation2.png\" width=\"400\">\n",
    "\n",
    "The simplest splitting strategy. A good choice when you have a lot of data. The **only** choice when you have time-dependent data. I use 60%-20%-20% when splitting data into training, validation, and test sets.\n",
    "\n",
    "*Shuffle* randomizes the order of rows before splitting. Do this is only if your data set does **not** depend on time.\n",
    "\n",
    "*Stratify* makes sure the ratio of classes is the same after splitting (stratify, thus, makes no sense for regressions). If building a classification model, always do this (I cannot think of situations when not to do it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a large data set, and split it\n",
    "X, y = np.concatenate([data['X_devel'], data['X_test']]), np.concatenate([data['y_devel'], data['y_test']])\n",
    "\n",
    "# do a 80-20 stratified shuffle split to create a development and a test set\n",
    "X_devel2, X_test2, y_devel2, y_test2 = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=THE_ANSWER)\n",
    "\n",
    "# now split the development set into a train and validation sets (stratify and shuffle)\n",
    "X_train2, X_valid2, y_train2, y_valid2 = train_test_split(X_devel2, y_devel2, test_size=0.25, random_state=THE_ANSWER)\n",
    "\n",
    "# make sure we got the right ratios\n",
    "print('The train-valid-test split is %.0f%%-%.0f%%-%.0f%%.' % (100*y_train2.size/float(y.size), 100*y_valid2.size/float(y.size), 100*y_test2.size/float(y.size)))\n",
    "\n",
    "print('The percentage of class == 1 objects in train, valid, and test sets is %.0f%%, %.0f%%, and %.0f%%.' % (100*y_train2.sum()/float(y_train2.size),\n",
    "                                                                                          100*y_valid2.sum()/float(y_valid2.size),\n",
    "                                                                                          100*y_test2.sum()/float(y_test2.size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2 k-fold cross-validation (CV)<a name=\"4_4_2\"></a>\n",
    "<img src=\"../img/4/k-fold_cross-validation.png\" width=\"600\">[Source](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf)\n",
    "\n",
    "A more complicated splitting strategy. The input data set is optionally shuffled and then split into *k* equal-sized non-overlapping groups (with the same ratio of classes, if stratified splitting is used). Each of these groups acts as a validation set (shown in beige), and the remainder as a training set (shown in blue). The test error is estimated by averaging the five resulting scoring estimates (e.g., area under the precision-recall curve).\n",
    "\n",
    "**Recommendation**: 5 to 10 folds, shuffle (use a fixed seed for reproducibility), and stratify (if building a classification model).\n",
    "\n",
    "**Example**: Use the above-defined Random Forest classification model, and measure it's performance using 5-fold shuffled and stratified CV on the *X_train_fixed* data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure average AUPRC using 5-fold shuffled and stratified cross-validation\n",
    "#cv_scores = cross_val_score(model, X_train_fixed, y_train, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=THE_ANSWER), n_jobs=1, scoring='average_precision')\n",
    "#print('The average CV score is %.4f, and its standard deviation is %.4f' % (np.average(cv_scores), np.std(cv_scores, ddof=1)))\n",
    "print('The average CV score is %.4f, and its standard deviation is %.4f' % (0.9910, 0.0004))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What advantages does k-fold cross-validation have over hold-out validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"4_4_3\"></a>4.4.3 Why shuffling and cross-validation are very, very bad when using time-dependent data (e.g., financial data)\n",
    "\n",
    "In [Section 4.2.3](#4_2_3), we trained a Random Forest classifier on a training set (*X_train_fixed*), and evaluated the model on the test set (*X_test_fixed*) using the average precision (or AUPRC) metric. We measured AUPRC = 0.923. By doing a 5-fold shuffled cross-validation on *X_train_fixed* we measured the average AUPRC of 0.991.\n",
    "\n",
    "Is the model evaluated using cross-validation really better than the one evaluated using hold-out validation?\n",
    "\n",
    "**NO!**\n",
    "\n",
    "The data used in this notebook are **time-dependent**. By shuffling the rows in *X_train_fixed*, we have allowed the algorithm to train on some \"future\" data points, and have enabled it to learn more than it normally could. The result is an overly optimistic evaluation of the trained model.\n",
    "\n",
    "**Recommendation**: When having time-stamped input data (i.e., rows) that you suspect may be generated by a time-dependent process, use hold-out validation and make sure your training, validation, and test data sets are sorted in that order in time and **do not** overlap in time.\n",
    "\n",
    "**Question**: What other splitting strategies can you think of that honor the time-ordering of input data, and return more than one test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: What have we learned so far?<a name=\"recap\"></a>\n",
    "1. How to train a Random Forest classifier.\n",
    "2. When the data set contains imbalanced classes, use the area under the precision-recall curve instead of accuracy when evaluating binary classification models. Use log loss for multi-class problems.\n",
    "3. **Be very careful** when splitting time-dependend data into training, validation, and test sets. Otherwise, use a shuffled k-fold cross-validation (stratified, if evaluating a classifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Optimization of model hyper-parameters<a name=\"4_5\"></a>\n",
    "\n",
    "At the end of [Section 4.2.3](#4_2_3), we have evaluated the classifier and measured AUPRC = 0.923. We can attempt to further improve the classifier by tuning the model hyper-parameters.\n",
    "\n",
    "To evaluate the performance of a set of hyper-parameters, we will use the **validation** set. Once we find the optimal set of hyper-parameters, we will test the final model on the **test** set.\n",
    "\n",
    "**Note:** Since the input data are time-dependent, I made sure the training, validation, and test sets are properly ordered in time (i.e, training &#9658; validation &#9658; test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the validation set\n",
    "X_valid, y_valid = data['X_valid'], data['y_valid']\n",
    "\n",
    "# replace NaNs with 10000\n",
    "X_valid_fixed = X_valid.copy()\n",
    "X_valid_fixed[np.isnan(X_valid_fixed)] = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## enter and execute your code here\n",
    "# instantiate the RandomForestClassifier\n",
    "\n",
    "\n",
    "# show the model\n",
    "\n",
    "\n",
    "# set the number of trees to 10, and the random seed to THE_ANSWER for reproducibility\n",
    "\n",
    "\n",
    "# fit the model using training data (X_train_fixed and y_train)\n",
    "\n",
    "\n",
    "# push the test data set (X_valid_fixed) through the model to get the classification *scores*, not labels\n",
    "# (i.e., use the \"predict_proba\" method)\n",
    "#y_scores_new = model_new.predict_proba(X_valid_fixed)\n",
    "\n",
    "\n",
    "# calculate the average precision (AUPRC) for class == 1 objects\n",
    "# by feeding the true class label (y_valid) and the predicted classification scores (y_scores_new[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.cell.code_cell.rendered.selected div.input').hide();\n",
       " } else {\n",
       " $('div.cell.code_cell.rendered.selected div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "The solution to the above task can be seen <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.cell.code_cell.rendered.selected div.input').hide();\n",
    " } else {\n",
    " $('div.cell.code_cell.rendered.selected div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The solution to the above task can be seen <a href=\"javascript:code_toggle()\">here</a>.''')\n",
    "\n",
    "# instantiate the RandomForestClassifier\n",
    "# uncomment the line below\n",
    "#model_new = RandomForestClassifier()\n",
    "\n",
    "# show the model\n",
    "# uncomment the line below\n",
    "#print(model_new)\n",
    "\n",
    "# set the number of trees to 10, and the random seed to THE_ANSWER for reproducibility\n",
    "# uncomment the line below\n",
    "#model_new.set_params(n_estimators=10, random_state=THE_ANSWER)\n",
    "\n",
    "# fit the model using training data (X_train_fixed and y_train)\n",
    "# uncomment the line below\n",
    "#model_new.fit(X_train_fixed, y_train)\n",
    "\n",
    "# push the test data set (X_valid_fixed) through the model to get the classification *scores*, not labels\n",
    "# (i.e., use the \"predict_proba\" method)\n",
    "# uncomment the line below\n",
    "#y_scores_new = model_new.predict_proba(X_valid_fixed)\n",
    "\n",
    "# calculate the average precision (AUPRC) for class == 1 objects\n",
    "# by feeding the true class label (y_valid) and the predicted classification scores (y_scores_new[:, 1])\n",
    "# uncomment the line below\n",
    "#print('The area under the precision-recall curve is %.3f.' % average_precision_score(y_valid, y_scores_new[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of manully finding the optimal model, one can use the sklearn [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# due to the RandomizedSearchCV \"cv\" argument,\n",
    "# first we need to merge train and validation sets...\n",
    "X_devel = np.concatenate([X_train_fixed, X_valid_fixed])\n",
    "y_devel = np.concatenate([y_train, y_valid])\n",
    "\n",
    "# ...and then provide indices into them \n",
    "train_idx = np.arange(y_devel.size)[0:y_train.size]\n",
    "valid_idx = np.arange(y_devel.size)[y_train.size:]\n",
    "\n",
    "# check that the indices make sense\n",
    "print('The last 5 elements of the train_idx:', train_idx[-5:])\n",
    "print('The first 5 elements of the valid_idx:', valid_idx[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to report best scores\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\"max_depth\": [3, 5, 15, 30, None],\n",
    "              \"max_features\": sp_randint(1, 11),\n",
    "              \"min_samples_split\": sp_randint(2, 11),\n",
    "              \"min_samples_leaf\": sp_randint(1, 11),\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# instantiate the classifier\n",
    "model = RandomForestClassifier(n_estimators=10, random_state=THE_ANSWER, n_jobs=4)\n",
    "\n",
    "# instantiate RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_dist,\n",
    "                                   n_iter=10, cv=[(train_idx, valid_idx)])\n",
    "\n",
    "# find the optimal model\n",
    "random_search.fit(X_devel, y_devel)\n",
    "\n",
    "# print the report\n",
    "report(random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now evaluate the optimal model on the test set\n",
    "# optimal model\n",
    "opt_model = random_search.best_estimator_\n",
    "print(opt_model)\n",
    "print()\n",
    "\n",
    "# use the devel set (train+valid) for training\n",
    "opt_model.fit(X_devel, y_devel)\n",
    "\n",
    "# evaluate on the test set\n",
    "y_scores_final = opt_model.predict_proba(X_test_fixed)\n",
    "print('The area under the precision-recall curve is %.3f.' % average_precision_score(y_test, y_scores_final[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomizedSearchCV just evaluates models for randomly selected hyper-paramater sets, which is time-consuming. To search the parameter space in a more efficient manner use [HyperOpt](https://hyperopt.github.io/hyperopt/), [SMAC3](https://github.com/automl/SMAC3), or [scikit-optimize](https://github.com/scikit-optimize/scikit-optimize) packages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
