{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation\n",
    "In machine learning, model validation is referred to as the process where a trained model is evaluated with a testing data set. The testing data set is a separate portion of the same data set from which the training set is derived. The main purpose of using the testing data set is to test the generalization ability of a trained model ([Alpaydin 2010](http://scholar.google.com/scholar_lookup?title=Introduction%20to%20machine%20learning&author=E.%20Alpaydin&publication_year=2010)).\n",
    " \n",
    " In simpler terms, model evaluation tells you what kind of performance you can expect once your run **future** data through the model.\n",
    " \n",
    " The outline of the model building and evaluation process is given below.\n",
    " <img src=\"../img/4/model_evaluation_process.PNG\" width=400>\n",
    " \n",
    " We will start by training a classification model that uses the [scikit-learn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) of the [Random Forest](https://en.wikipedia.org/wiki/Random_forest) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load necessary Python packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, TimeSeriesSplit\n",
    "from sklearn.metrics import average_precision_score, accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve, precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# data are stored here\n",
    "data_folder = '../datasets'\n",
    "\n",
    "# integer for the random seed\n",
    "THE_ANSWER = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all data\n",
    "data = np.load('%s/data.npz' % data_folder)\n",
    "\n",
    "# extract training data\n",
    "X_train, y_train = data['X_train'], data['y_train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *X_train* 2-d array 33 contains features (columns) that describe objects of interest. Based on my experience, about 95% of project time is spent writing code to create this array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at the class labels stored in the y_train array\n",
    "print(y_train)\n",
    "\n",
    "# only 34% of objects are of class 1 --> the data set is imbalanced\n",
    "print('The class == 1 objects make %0.f%% of the training set.' % (100*y_train.sum()/y_train.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# print out the default values of model hyper-parameters (will talk about hyper-parameters later)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number of trees to 500, and the random seed to THE_ANSWER for reproducibility\n",
    "model.set_params(n_estimators=500, random_state=THE_ANSWER)\n",
    "#model = RandomForestClassifier(n_estimators=500, random_state=THE_ANSWER) # another way of doing it\n",
    "\n",
    "# check that the model changed\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some values in the training set are NaN (i.e., are missing).\n",
    "# The Random Forest classifier does not like that, so we set NaN values to 10000.\n",
    "X_train_fixed = X_train.copy()\n",
    "X_train_fixed[np.isnan(X_train_fixed)] = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model using training data\n",
    "model.fit(X_train_fixed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification metrics\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "Now that we have fit a model, we need to evaluate its performance.\n",
    "That can be done using various [classification metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics), but the simplest one is accuracy.\n",
    "Accuracy is defined as the percentage of correct predictions for the test data.\n",
    "\n",
    "$$accuracy = \\frac{{\\rm number\\, of\\, correct\\, predictions}}{{\\rm number\\, of\\, all\\, predictions}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, extract test data\n",
    "X_test, y_test = data['X_test'], data['y_test']\n",
    "\n",
    "# if there are NaNs in the test set, set them to 10000.\n",
    "X_test_fixed = X_test.copy()\n",
    "X_test_fixed[np.isnan(X_test)] = 10000\n",
    "\n",
    "# only 34% of objects are of class 1 --> the data set is imbalanced\n",
    "print('The class == 1 objects make %0.f%% of the test set.' % (100*y_test.sum()/y_test.size))\n",
    "\n",
    "# now push the test set through the model to get the predicted classes\n",
    "y_pred = model.predict(X_test_fixed)\n",
    "\n",
    "print('Predicted class:', y_pred)\n",
    "print('True class:     ', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the accuracy\n",
    "print('The accuracy is %.3f.' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a great result, right? Maybe not since we have an imbalanced data set (i.e., class == 1 objects make only 35% of the test set). Here's an extreme example how accuracy can be misleading when the classes are imbalanced:\n",
    "\n",
    "Imagine a data set where 90% of objects are of class 0. A naive classifier that predicts 0 for all objects will have an accuracy of 90%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall\n",
    "\n",
    "In my opinion, what most business people would like to know is the following:\n",
    "1. What is the detection rate? Did we correctly identify 50%, 80%, or more of all class 1 objects (e.g., instructions that will fail to settle)?\n",
    "2. What is the false alarm rate? What percentage of those tagged as class 1 are **not** of class 1?\n",
    "\n",
    "The detection rate is called [\"recall\"](https://en.wikipedia.org/wiki/Precision_and_recall) in machine learning, and the false alarm rate is related to [\"precision\"](https://en.wikipedia.org/wiki/Precision_and_recall) (actually, it is 1-precision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENIA: use graphics on this page to explain the confusion matrix, precision, and recall\n",
    "# https://www.jeremyjordan.me/evaluating-a-machine-learning-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand the confusion matrix, precision, and recall, let's calculate them using the test set for the model we just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = precision_score(y_test, y_pred)\n",
    "r = recall_score(y_test, y_pred)\n",
    "print('The precision obtained on the test set is %.0f%% and recall is %.0f%%.' % (p*100, r*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above precision and recall indicate that our model is able to detect 83% of all class == 1 objects (e.g., failed settlements), while having a false alarm rate of only 14% (remember that the false alarm rate = 1 - precision).\n",
    "\n",
    "But what if a detection rate of 86% is not good enough for your use case? What if you willing to accept a higher false alarm rate if that would enable you to identify more of class == 1 objects (i.e., have a higher detection rate)?\n",
    "\n",
    "This can be achieved by:\n",
    "1. Calculating classification **scores** (values 0 **to** 1) instead of predicting class **labels** (values 0 **or** 1),\n",
    "2. Plotting the precision-recall curve, and\n",
    "3. Selecting all objects that have the classification score higher than some threshold.\n",
    "\n",
    "Let's start by calculating and explaining classification scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push the test data set through the model to get the classification *scores* (not labels)\n",
    "# (note that we use the \"predict_proba\" method and not \"predict\")\n",
    "y_scores = model.predict_proba(X_test_fixed)\n",
    "\n",
    "# the model calculates the score for both classes (0 and 1). Each row sums to 1.\n",
    "print(y_scores)\n",
    "print('The y_scores is a 2-dimensional array:', y_scores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have classification scores, let's plot the (1-precision) vs. recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the detection rate vs. false alarm rate\n",
    "p, r, thresh = precision_recall_curve(y_test, y_scores[:, 1])\n",
    "thresh = np.append(0, thresh)\n",
    "print('Score > %.3f:  %.0f%% detection rate at 10%% false alarm rate.' % (thresh[(1-p) > 0.1][-1], 100*r[(1-p) > 0.1][-1]))\n",
    "print('Score > %.3f:  %.0f%% detection rate at 15%% false alarm rate.' % (thresh[(1-p) > 0.15][-1], 100*r[(1-p) > 0.15][-1]))\n",
    "print('Score > %.3f:  %.0f%% detection rate at 20%% false alarm rate.' % (thresh[(1-p) > 0.2][-1], 100*r[(1-p) > 0.2][-1]))\n",
    "print('Score > 0.000: 100%% detection rate at %.0f%% false alarm rate (naive approach).' % (100-100*y_test.sum()/y_test.size))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.step((1-p)*100, r*100, color='b', where='post', label='late settlements')\n",
    "ax.set_xlabel('False alarm rate [%]')\n",
    "ax.set_ylabel('Detection rate [%]')\n",
    "ax.set_ylim([0.0, 101])\n",
    "_ = ax.set_xlim([0.0, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, if we select all objects (rows) that have the classification score (y_scores[:, 1]) greater than 0.557, we can expect a sample where 78% of objects will be of class 1 (e.g., will be failed settlements), and only 10% will be of class 0.\n",
    "\n",
    "Example: Let's say there are 100 objects (rows) that have the classification score greater than 0.577. This mean that 90 objects will truly be of class 1 (false alarm rate is 10% --> precision is 90% --> 100*0.9 = 90). The total number of class 1 objects is 115 ( = 90/0.78 --> because detection rate is 78%), which means the algorithm misses 25 objects (rows) of class 1.\n",
    "\n",
    "The area under the precision-recall curve (AUPRC) is also a [metric](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html). The perfect classifier (model) has a detection rate of 100% and false alarm rate of 0%, and the AUPRC = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the area under the precision-recall curve for class == 1 objects\n",
    "# by feeding the true class label (y_test) and the predicted classification scores (y_scores[:, 1])\n",
    "print('The area under the precision-recall curve is %.3f.' % average_precision_score(y_test, y_scores[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log loss or cross-entropy\n",
    "\n",
    "The metric of choice for evaluating multi-class classification models. For more details, see [here](http://wiki.fast.ai/index.php/Log_Loss) and [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html#sklearn.metrics.log_loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression metrics\n",
    "\n",
    "**(Root) Mean Squared Error** is simply defined as the (root of the) average of squared differences between the predicted output and the true output. A smaller value of this metric indicates a better model. The root of MSE is in units of *y*.\n",
    "$$MSE(y_{true}, y_{pred})=\\frac{1}{n_{samples}}\\sum{(y_{true} - y_{pred})^2}$$\n",
    "\n",
    "[${\\bf R^2}$ **coefficient**](https://en.wikipedia.org/wiki/Coefficient_of_determination) or the **coefficient of determination** represents the proportion of variance in the outcome that our model is capable of predicting based on its features. The values range from 1 (perfect model) to minus infinity (the model can be infinitely bad).\n",
    "$$R^2(y_{true}, y_{pred}) = 1 - \\frac{\\sum{(y_{true} - y_{pred})^2}}{\\sum{(y_{true} - \\bar{y})^2}}; \\bar{y} = \\frac{1}{n_{samples}}\\sum{y_{true}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies for splitting data into train, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(model, X_train_fixed, y_train, cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=THE_ANSWER), n_jobs=4, scoring='average_precision')\n",
    "print('Average CV score is %.4f, and the rms scatter is %.4f' % (np.average(cv_scores), np.std(cv_scores, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, y_valid = data['X_valid'], data['y_valid']\n",
    "X_devel, y_devel = data['X_devel'], data['y_devel']\n",
    "X_test, y_test = data['X_test'], data['y_test']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
