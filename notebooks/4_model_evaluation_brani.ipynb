{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Model Evaluation\n",
    "confusion matrix, hyperparameter optimization (random forests), cross validation, train_test_split (should have been done before?), metrices, accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, TimeSeriesSplit\n",
    "from sklearn.metrics import average_precision_score, accuracy_score, precision_recall_curve\n",
    "\n",
    "# data are stored here\n",
    "data_folder = 'C:/Users/Branimir/AnacondaProjects/usecase_0056/Output/DE0001102390'\n",
    "\n",
    "# integer for the random seed\n",
    "THE_ANSWER = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = np.load('%s/data.npz' % data_folder)\n",
    "\n",
    "# extract training data\n",
    "X_train, y_train = data['X_train'], data['y_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 1. ... 0. 0. 0.]\n",
      "The class == 1 rows make 34% of the data set.\n"
     ]
    }
   ],
   "source": [
    "# let's take a look at the class labels (i.e., the y_train array)\n",
    "print(y_train)\n",
    "\n",
    "# only 34% of rows are of class 1 --> the data set is imbalanced\n",
    "print('The class == 1 rows make %0.f%% of the data set.' % (100*y_train.sum()/y_train.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# print out the default values of model hyper-parameters (will talk about hyper-parameters later)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=None,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the number of trees to 500, and the random seed to THE_ANSWER for reproducibility\n",
    "model.set_params(n_estimators=500, random_state=THE_ANSWER)\n",
    "#model = RandomForestClassifier(n_estimators=500, random_state=THE_ANSWER) # another way of doing it\n",
    "\n",
    "# check that the model changed\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some values in the training set are NaN (i.e., are missing).\n",
    "# The Random Forest classifier does not like that, so we set NaN values to 10000.\n",
    "X_train_fixed = X_train.copy()\n",
    "X_train_fixed[np.isnan(X_train_fixed)] = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=None,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model using training data\n",
    "model.fit(X_train_fixed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we have fit the model, we need to evaluate its performance\n",
    "# that can be done using various metrics, but the simplest one is accuracy.\n",
    "# Accuracy is defined as the percentage of correct predictions for the test data.\n",
    "# accuracy = number of correct predictions / number of all predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: [0. 0. 0. ... 1. 0. 0.]\n",
      "True class:      [1. 0. 0. ... 1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "## calculate accuracy on the test set\n",
    "# set NaNs to 10000\n",
    "X_test_fixed = X_test.copy()\n",
    "X_test_fixed[np.isnan(X_test)] = 10000\n",
    "\n",
    "# push the test data set through the model to get the classification scores\n",
    "y_scores = model.predict(X_test_fixed)\n",
    "\n",
    "# the model predicts the class.\n",
    "print('Predicted class:', y_scores)\n",
    "print('True class:     ', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.892.\n"
     ]
    }
   ],
   "source": [
    "# calculate the accuracy\n",
    "print('The accuracy is %.3f.' % accuracy_score(y_test, y_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great result, right? May not be since we have an imbalanced data set.\n",
    "# Here's an extreme example: Imagine a data set where 90% of rows are of class 0.\n",
    "# A naive classifier that predicts 0 for all rows will have an accuracy of 90%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In my opinion, what most business people would like to know is the following:\n",
    "# 1) What is the detection rate? Did we correctly identify 50%, 80%, or more of all class 1 objects?\n",
    "# 2) What is the false alarm rate? What percentage of those tagged as class 1 are *not* class 1?\n",
    "# The detection rate is called \"recall\" in machine learning,\n",
    "# and the false alarm rate is related to \"precision\" (actually, it is 1-precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use graphics on this page to explain the confusion matrix, precision, and recall\n",
    "# https://www.jeremyjordan.me/evaluating-a-machine-learning-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.88866667 0.11133333]\n",
      " [0.828      0.172     ]\n",
      " [0.85693333 0.14306667]\n",
      " ...\n",
      " [0.378      0.622     ]\n",
      " [0.584      0.416     ]\n",
      " [0.804      0.196     ]]\n",
      "The y_scores is a 2-dimensional array. (22034, 2)\n"
     ]
    }
   ],
   "source": [
    "# push the test data set through the model to get the classification *scores* (not labels)\n",
    "# (note that we use the \"predict_proba\" method and not \"predict\")\n",
    "y_scores = model.predict_proba(X_test_fixed)\n",
    "\n",
    "# the model calculates the score for both classes (0 and 1). Each row sums to 1.\n",
    "print(y_scores)\n",
    "print('The y_scores is a 2-dimensional array:', y_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under the Precision-Recall curve is 0.923.\n"
     ]
    }
   ],
   "source": [
    "# calculate the area under the precision-recall curve for the \"positive\" class\n",
    "# by feeding the true class label (y_test) and the predicted classification scores (y_scores[:, 1])\n",
    "print('The area under the Precision-Recall curve is %.3f.' % average_precision_score(y_test, y_scores[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different metrics for evaluating classification models can be found here:\n",
    "# https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, y_valid = data['X_valid'], data['y_valid']\n",
    "X_devel, y_devel = data['X_devel'], data['y_devel']\n",
    "X_test, y_test = data['X_test'], data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65579, 33)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
